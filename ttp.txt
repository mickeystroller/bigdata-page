Topic 5
Ix-cubes: iceberg cubes for data warehousing and olap on xml data
With increasing amount of data being stored in XML format, OLAP queries over these data become important. OLAP queries have been well studied in the relational database systems. However, the evaluation of OLAP queries over XML data is not a trivial extension of the relational solutions, especially when a schema is not available. In this paper, we introduce the IX-cube (Iceberg XML cube) over XML data to tackle the problem. We extend OLAP operations to XML data. We also develop efficient approaches to IX-Cube computation and OLAP query evaluation using IX-cubes.

Advanced Techniques for Scientific Data Warehouses
Data warehouses using a multidimensional view of data have become very popular in both business and science in recent years. Data warehouses for scientific purposes such as medicine and bio-chemistry1 pose several great challenges to existing data warehouse technology. Data warehouses usually use pre-aggregated data to ensure fast query response. However, pre-aggregation cannot be used in practice if the dimension structures or the relationships between facts and dimensions are irregular. A technique for overcoming this limitation and some experimental results are presented. Queries over scientific data warehouses often need to reference data that is external to the data warehouse, e.g., data that is too complex to be handled by current data warehouse technology, data that is ”owned” by other organizations, or data that is updated frequently. This paper presents a federation architecture that allows the integration of multidimensional warehouse data with complex external data.

Present and future directions in data warehousing
Many large organizations have developed data warehouses to support decision making. The data in a warehouse are subject oriented, integrated, time variant, and nonvolatile. A data warehouse contains five types of data: current detail data, older detail data, lightly summarized data, highly summarized data, and metadata. The architecture of a data warehouse includes a backend process (the extraction of data from source systems), the warehouse, and the front-end use (the accessing of data from the warehouse). A data mart is a smaller version of a data warehouse that supports the narrower set of requirements of a single business unit. Data marts should be developed in an integrated manner in order to avoid repeating the "silos of information" problem.An operational data store is a database for transaction processing systems that uses the data warehouse approach to provide clean data. Data warehousing is constantly changing, with the associated opportunities for practice and research, such as the potential for knowledge management using the warehouse.

Maintenance of data cubes and summary tables in a warehouse
Data warehouses contain large amounts of information, often collected from a variety of independent sources. Decision-support functions in a warehouse, such as on-line analytical processing (OLAP), involve hundreds of complex aggregate queries over large volumes of data. It is not feasible to compute these queries by scanning the data sets each time. Warehouse applications therefore build a large number of summary tables, or materialized aggregate views, to help them increase the system performance. As changes, most notably new transactional data, are collected at the data sources, all summary tables at the warehouse that depend upon this data need to be updated. Usually, source changes are loaded into the warehouse at regular intervals, usually once a day, in a batch window, and the warehouse is made unavailable for querying while it is updated. Since the number of summary tables that need to be maintained is often large, a critical issue for data warehousing is how to maintain the summary tables efficiently. In this paper we propose a method of maintaining aggregate views (the summary-delta table method), and use it to solve two problems in maintaining summary tables in a warehouse: (1) how to efficiently maintain a summary table while minimizing the batch window needed for maintenance, and (2) how to maintain a large set of summary tables defined over the same base tables. While several papers have addressed the issues relating to choosing and materializing a set of summary tables, this is the first paper to address maintaining summary tables efficiently.

Pascal Poncelet, Florent Masseglia, Maguelonne Teisseire: Successes and New Directions in Data Mining
Null

Modeling and Executing the Data Warehouse Refreshment Process
Data warehouse refreshment is often viewed as a problem of maintaining materialized views over operational sources. In this paper, we show that the data warehouse refreshment process is a complex process comprising several tasks, e.g.,monitoring, extracting, transforming, integrating and cleaning operational data, deriving new data, building histories and loading the data warehouse. We propose a novel approach for defining and executing the refreshment process based on specifications stored in an object-oriented metadata repository. Our approach considers the multidimensional character of OLAP data and can be used in conjunction with various operational sources and target data warehouses.

Efficient Algorithms for On-line Analysis Processing On Compressed Data Warehouses
Data compression is an effective technique to improve the performance of data warehouses. Aggregation and cube are important operations for on-line analytical processing (OLAP). It is a major challenge to develop efficient algorithms for aggregation and cube operations on compressed data warehouses. Many efficient algorithms to compute aggregation and cube for relational OLAP have been developed. Some work has been done on efficiently computing aggregation and cube for multidimensional data warehouses (MDWs) that store datasets in multidimensional arrays rather than in tables. However, to our knowledge, there is few to date in the literature describing aggregation algorithms on compressed data warehouses for multidimensional OLAP.

Techniche di data mining per l'analisi di bilanci aziendali: data-mining techniques for the analysis of economic and financial data
Null

Kimball's Data Warehouse Toolkit Classics: The Data Warehouse Toolkit, 2nd Edition; The Data Warehouse Lifecycle, 2nd Edition; The Data Warehouse ETL Toolk, 2nd edition
Three books that set the standard with their groundbreaking methods and techniques for data warehousing. Great value! Save $20, value of the set is $165. This set includes the following books from the Kimball Group: The Data Warehouse Toolkit, Second Edition This authoritative guide presents clear-cut guidelines for designing dimensional models through the use of real-world data warehouse case studies drawn from a variety of business application areas and industries. Ralph Kimball and Margy Ross help you master the full range of powerful techniques for designing dimensional databases that are easy to understand and provide fast query responses. The Data Warehouse Lifecycle Toolkit, Second Edition With this second edition, Ralph Kimball and his Kimball Group colleagues refined the original set of Lifecycle methods and techniques based on their consulting and training experiences. They walk you through detailed steps of designing, developing, and deploying a data warehousing/business intelligence system, from business requirements gathering through delivery, with the steadfast goal of enabling users to make better, more informed business decisions. The Data Warehouse ETL Toolkit Serving as a road map for planning, designing, building, and running the backroom of a data warehouse, this book provides complete coverage of proven, time-saving ETL techniques. You'll discover how a properly designed ETL system extracts data from the source systems, enforces data quality and consistency standards, conforms the data so that separate sources can be used together, and finally delivers the data in a presentation-ready format

Storing semistructured data with STORED
Systems for managing and querying semistructured-data sources often store data in proprietary object repositories or in a tagged-text format. We describe a technique that can use relational database management systems to store and manage semistructured data. Our technique relies on a mapping between the semistructured data model and the relational data model, expressed in a query language called STORED. When a semistructured data instance is given, a STORED mapping can be generated automatically using data-mining techniques. We are interested in applying STORED to XML data, which is an instance of semistructured data. We show how a document-type-descriptor (DTD), when present, can be exploited to further improve performance.


Topic 6
Effects of Translational and Gripping Force Feedback are Decoupled in a 4-Degree-of-Freedom Telemanipulator
With increasing amount of data being stored in XML format, OLAP queries over these data become important. OLAP queries have been well studied in the relational database systems. However, the evaluation of OLAP queries over XML data is not a trivial extension of the relational solutions, especially when a schema is not available. In this paper, we introduce the IX-cube (Iceberg XML cube) over XML data to tackle the problem. We extend OLAP operations to XML data. We also develop efficient approaches to IX-Cube computation and OLAP query evaluation using IX-cubes.

Intuitionistic fuzzy Lie sub-superalgebras and intuitionistic fuzzy ideals
Data warehouses using a multidimensional view of data have become very popular in both business and science in recent years. Data warehouses for scientific purposes such as medicine and bio-chemistry1 pose several great challenges to existing data warehouse technology. Data warehouses usually use pre-aggregated data to ensure fast query response. However, pre-aggregation cannot be used in practice if the dimension structures or the relationships between facts and dimensions are irregular. A technique for overcoming this limitation and some experimental results are presented. Queries over scientific data warehouses often need to reference data that is external to the data warehouse, e.g., data that is too complex to be handled by current data warehouse technology, data that is ”owned” by other organizations, or data that is updated frequently. This paper presents a federation architecture that allows the integration of multidimensional warehouse data with complex external data.

The depressor trochanteris motorneurones and their role in the coxo-trochanteral feedback loop in the stick insect carausius morosus
Many large organizations have developed data warehouses to support decision making. The data in a warehouse are subject oriented, integrated, time variant, and nonvolatile. A data warehouse contains five types of data: current detail data, older detail data, lightly summarized data, highly summarized data, and metadata. The architecture of a data warehouse includes a backend process (the extraction of data from source systems), the warehouse, and the front-end use (the accessing of data from the warehouse). A data mart is a smaller version of a data warehouse that supports the narrower set of requirements of a single business unit. Data marts should be developed in an integrated manner in order to avoid repeating the "silos of information" problem.An operational data store is a database for transaction processing systems that uses the data warehouse approach to provide clean data. Data warehousing is constantly changing, with the associated opportunities for practice and research, such as the potential for knowledge management using the warehouse.

A Seven-degrees-of-freedom Robot-arm Driven by Pneumatic Artificial Muscles for Humanoid Robots
Data warehouses contain large amounts of information, often collected from a variety of independent sources. Decision-support functions in a warehouse, such as on-line analytical processing (OLAP), involve hundreds of complex aggregate queries over large volumes of data. It is not feasible to compute these queries by scanning the data sets each time. Warehouse applications therefore build a large number of summary tables, or materialized aggregate views, to help them increase the system performance. As changes, most notably new transactional data, are collected at the data sources, all summary tables at the warehouse that depend upon this data need to be updated. Usually, source changes are loaded into the warehouse at regular intervals, usually once a day, in a batch window, and the warehouse is made unavailable for querying while it is updated. Since the number of summary tables that need to be maintained is often large, a critical issue for data warehousing is how to maintain the summary tables efficiently. In this paper we propose a method of maintaining aggregate views (the summary-delta table method), and use it to solve two problems in maintaining summary tables in a warehouse: (1) how to efficiently maintain a summary table while minimizing the batch window needed for maintenance, and (2) how to maintain a large set of summary tables defined over the same base tables. While several papers have addressed the issues relating to choosing and materializing a set of summary tables, this is the first paper to address maintaining summary tables efficiently.

Dynamics modeling and analysis of a biped walking robot actuated by a closed-chain mechanism
Null

Fuzzy + PID Controller for Robot Manipulator
Data warehouse refreshment is often viewed as a problem of maintaining materialized views over operational sources. In this paper, we show that the data warehouse refreshment process is a complex process comprising several tasks, e.g.,monitoring, extracting, transforming, integrating and cleaning operational data, deriving new data, building histories and loading the data warehouse. We propose a novel approach for defining and executing the refreshment process based on specifications stored in an object-oriented metadata repository. Our approach considers the multidimensional character of OLAP data and can be used in conjunction with various operational sources and target data warehouses.

Locomotion control of a hydraulically actuated hexapod robot by robust adaptive fuzzy control and dead-zone compensation
Data compression is an effective technique to improve the performance of data warehouses. Aggregation and cube are important operations for on-line analytical processing (OLAP). It is a major challenge to develop efficient algorithms for aggregation and cube operations on compressed data warehouses. Many efficient algorithms to compute aggregation and cube for relational OLAP have been developed. Some work has been done on efficiently computing aggregation and cube for multidimensional data warehouses (MDWs) that store datasets in multidimensional arrays rather than in tables. However, to our knowledge, there is few to date in the literature describing aggregation algorithms on compressed data warehouses for multidimensional OLAP.

Locomotion Control of a Hydraulically Actuated Hexapod Robot by Robust Adaptive Fuzzy Control with Self-Tuned Adaptation Gain and Dead Zone Fuzzy Pre-compensation
Null

Force & torque feedback vs force only feedback
Three books that set the standard with their groundbreaking methods and techniques for data warehousing. Great value! Save $20, value of the set is $165. This set includes the following books from the Kimball Group: The Data Warehouse Toolkit, Second Edition This authoritative guide presents clear-cut guidelines for designing dimensional models through the use of real-world data warehouse case studies drawn from a variety of business application areas and industries. Ralph Kimball and Margy Ross help you master the full range of powerful techniques for designing dimensional databases that are easy to understand and provide fast query responses. The Data Warehouse Lifecycle Toolkit, Second Edition With this second edition, Ralph Kimball and his Kimball Group colleagues refined the original set of Lifecycle methods and techniques based on their consulting and training experiences. They walk you through detailed steps of designing, developing, and deploying a data warehousing/business intelligence system, from business requirements gathering through delivery, with the steadfast goal of enabling users to make better, more informed business decisions. The Data Warehouse ETL Toolkit Serving as a road map for planning, designing, building, and running the backroom of a data warehouse, this book provides complete coverage of proven, time-saving ETL techniques. You'll discover how a properly designed ETL system extracts data from the source systems, enforces data quality and consistency standards, conforms the data so that separate sources can be used together, and finally delivers the data in a presentation-ready format

Robust Impedance Control of a Delayed Telemanipulator Considering Hysteresis Nonlinearity of the Piezo-actuated Slave Robot
Systems for managing and querying semistructured-data sources often store data in proprietary object repositories or in a tagged-text format. We describe a technique that can use relational database management systems to store and manage semistructured data. Our technique relies on a mapping between the semistructured data model and the relational data model, expressed in a query language called STORED. When a semistructured data instance is given, a STORED mapping can be generated automatically using data-mining techniques. We are interested in applying STORED to XML data, which is an instance of semistructured data. We show how a document-type-descriptor (DTD), when present, can be exploited to further improve performance.


Topic 9
Non-cancellation multistage kurtosis maximization with prewhitening for blind source separation
With increasing amount of data being stored in XML format, OLAP queries over these data become important. OLAP queries have been well studied in the relational database systems. However, the evaluation of OLAP queries over XML data is not a trivial extension of the relational solutions, especially when a schema is not available. In this paper, we introduce the IX-cube (Iceberg XML cube) over XML data to tackle the problem. We extend OLAP operations to XML data. We also develop efficient approaches to IX-Cube computation and OLAP query evaluation using IX-cubes.

Analyzing variants of Shellsort
Data warehouses using a multidimensional view of data have become very popular in both business and science in recent years. Data warehouses for scientific purposes such as medicine and bio-chemistry1 pose several great challenges to existing data warehouse technology. Data warehouses usually use pre-aggregated data to ensure fast query response. However, pre-aggregation cannot be used in practice if the dimension structures or the relationships between facts and dimensions are irregular. A technique for overcoming this limitation and some experimental results are presented. Queries over scientific data warehouses often need to reference data that is external to the data warehouse, e.g., data that is too complex to be handled by current data warehouse technology, data that is ”owned” by other organizations, or data that is updated frequently. This paper presents a federation architecture that allows the integration of multidimensional warehouse data with complex external data.

Efficient algorithms for discrete time-frequency distributions
Many large organizations have developed data warehouses to support decision making. The data in a warehouse are subject oriented, integrated, time variant, and nonvolatile. A data warehouse contains five types of data: current detail data, older detail data, lightly summarized data, highly summarized data, and metadata. The architecture of a data warehouse includes a backend process (the extraction of data from source systems), the warehouse, and the front-end use (the accessing of data from the warehouse). A data mart is a smaller version of a data warehouse that supports the narrower set of requirements of a single business unit. Data marts should be developed in an integrated manner in order to avoid repeating the "silos of information" problem.An operational data store is a database for transaction processing systems that uses the data warehouse approach to provide clean data. Data warehousing is constantly changing, with the associated opportunities for practice and research, such as the potential for knowledge management using the warehouse.

Performance of a Class of Highly-Parallel Divide-and-Conquer Algorithms
Data warehouses contain large amounts of information, often collected from a variety of independent sources. Decision-support functions in a warehouse, such as on-line analytical processing (OLAP), involve hundreds of complex aggregate queries over large volumes of data. It is not feasible to compute these queries by scanning the data sets each time. Warehouse applications therefore build a large number of summary tables, or materialized aggregate views, to help them increase the system performance. As changes, most notably new transactional data, are collected at the data sources, all summary tables at the warehouse that depend upon this data need to be updated. Usually, source changes are loaded into the warehouse at regular intervals, usually once a day, in a batch window, and the warehouse is made unavailable for querying while it is updated. Since the number of summary tables that need to be maintained is often large, a critical issue for data warehousing is how to maintain the summary tables efficiently. In this paper we propose a method of maintaining aggregate views (the summary-delta table method), and use it to solve two problems in maintaining summary tables in a warehouse: (1) how to efficiently maintain a summary table while minimizing the batch window needed for maintenance, and (2) how to maintain a large set of summary tables defined over the same base tables. While several papers have addressed the issues relating to choosing and materializing a set of summary tables, this is the first paper to address maintaining summary tables efficiently.

An inverted taxonomy of sorting algorithms
Null

On the Complexity of Sparse Gaussian Elimination Via Bordering
Data warehouse refreshment is often viewed as a problem of maintaining materialized views over operational sources. In this paper, we show that the data warehouse refreshment process is a complex process comprising several tasks, e.g.,monitoring, extracting, transforming, integrating and cleaning operational data, deriving new data, building histories and loading the data warehouse. We propose a novel approach for defining and executing the refreshment process based on specifications stored in an object-oriented metadata repository. Our approach considers the multidimensional character of OLAP data and can be used in conjunction with various operational sources and target data warehouses.

Computation of absolute distance in the mongolian Gerbil (meriones unguiculatus): depth algorithms and neural substrates
Data compression is an effective technique to improve the performance of data warehouses. Aggregation and cube are important operations for on-line analytical processing (OLAP). It is a major challenge to develop efficient algorithms for aggregation and cube operations on compressed data warehouses. Many efficient algorithms to compute aggregation and cube for relational OLAP have been developed. Some work has been done on efficiently computing aggregation and cube for multidimensional data warehouses (MDWs) that store datasets in multidimensional arrays rather than in tables. However, to our knowledge, there is few to date in the literature describing aggregation algorithms on compressed data warehouses for multidimensional OLAP.

QuickHeapsort, an Efficient Mix of Classical Sorting Algorithms
Null

A class of sorting algorithms based on Quicksort
Three books that set the standard with their groundbreaking methods and techniques for data warehousing. Great value! Save $20, value of the set is $165. This set includes the following books from the Kimball Group: The Data Warehouse Toolkit, Second Edition This authoritative guide presents clear-cut guidelines for designing dimensional models through the use of real-world data warehouse case studies drawn from a variety of business application areas and industries. Ralph Kimball and Margy Ross help you master the full range of powerful techniques for designing dimensional databases that are easy to understand and provide fast query responses. The Data Warehouse Lifecycle Toolkit, Second Edition With this second edition, Ralph Kimball and his Kimball Group colleagues refined the original set of Lifecycle methods and techniques based on their consulting and training experiences. They walk you through detailed steps of designing, developing, and deploying a data warehousing/business intelligence system, from business requirements gathering through delivery, with the steadfast goal of enabling users to make better, more informed business decisions. The Data Warehouse ETL Toolkit Serving as a road map for planning, designing, building, and running the backroom of a data warehouse, this book provides complete coverage of proven, time-saving ETL techniques. You'll discover how a properly designed ETL system extracts data from the source systems, enforces data quality and consistency standards, conforms the data so that separate sources can be used together, and finally delivers the data in a presentation-ready format

An efficient divide and conquer algorithm for exact hazard free logic minimization
Systems for managing and querying semistructured-data sources often store data in proprietary object repositories or in a tagged-text format. We describe a technique that can use relational database management systems to store and manage semistructured data. Our technique relies on a mapping between the semistructured data model and the relational data model, expressed in a query language called STORED. When a semistructured data instance is given, a STORED mapping can be generated automatically using data-mining techniques. We are interested in applying STORED to XML data, which is an instance of semistructured data. We show how a document-type-descriptor (DTD), when present, can be exploited to further improve performance.


Topic 13
Critically sampled and oversampled complex filter banks via interleaved DFT modulation
With increasing amount of data being stored in XML format, OLAP queries over these data become important. OLAP queries have been well studied in the relational database systems. However, the evaluation of OLAP queries over XML data is not a trivial extension of the relational solutions, especially when a schema is not available. In this paper, we introduce the IX-cube (Iceberg XML cube) over XML data to tackle the problem. We extend OLAP operations to XML data. We also develop efficient approaches to IX-Cube computation and OLAP query evaluation using IX-cubes.

Signal De-noising in Ultrasonic Testing Based on Stationary Wavelet Transform
Data warehouses using a multidimensional view of data have become very popular in both business and science in recent years. Data warehouses for scientific purposes such as medicine and bio-chemistry1 pose several great challenges to existing data warehouse technology. Data warehouses usually use pre-aggregated data to ensure fast query response. However, pre-aggregation cannot be used in practice if the dimension structures or the relationships between facts and dimensions are irregular. A technique for overcoming this limitation and some experimental results are presented. Queries over scientific data warehouses often need to reference data that is external to the data warehouse, e.g., data that is too complex to be handled by current data warehouse technology, data that is ”owned” by other organizations, or data that is updated frequently. This paper presents a federation architecture that allows the integration of multidimensional warehouse data with complex external data.

Optimization of filter banks using cyclostationary spectral analysis
Many large organizations have developed data warehouses to support decision making. The data in a warehouse are subject oriented, integrated, time variant, and nonvolatile. A data warehouse contains five types of data: current detail data, older detail data, lightly summarized data, highly summarized data, and metadata. The architecture of a data warehouse includes a backend process (the extraction of data from source systems), the warehouse, and the front-end use (the accessing of data from the warehouse). A data mart is a smaller version of a data warehouse that supports the narrower set of requirements of a single business unit. Data marts should be developed in an integrated manner in order to avoid repeating the "silos of information" problem.An operational data store is a database for transaction processing systems that uses the data warehouse approach to provide clean data. Data warehousing is constantly changing, with the associated opportunities for practice and research, such as the potential for knowledge management using the warehouse.

Demodulation of noisy phase or frequency modulated signals with Kalman filters
Data warehouses contain large amounts of information, often collected from a variety of independent sources. Decision-support functions in a warehouse, such as on-line analytical processing (OLAP), involve hundreds of complex aggregate queries over large volumes of data. It is not feasible to compute these queries by scanning the data sets each time. Warehouse applications therefore build a large number of summary tables, or materialized aggregate views, to help them increase the system performance. As changes, most notably new transactional data, are collected at the data sources, all summary tables at the warehouse that depend upon this data need to be updated. Usually, source changes are loaded into the warehouse at regular intervals, usually once a day, in a batch window, and the warehouse is made unavailable for querying while it is updated. Since the number of summary tables that need to be maintained is often large, a critical issue for data warehousing is how to maintain the summary tables efficiently. In this paper we propose a method of maintaining aggregate views (the summary-delta table method), and use it to solve two problems in maintaining summary tables in a warehouse: (1) how to efficiently maintain a summary table while minimizing the batch window needed for maintenance, and (2) how to maintain a large set of summary tables defined over the same base tables. While several papers have addressed the issues relating to choosing and materializing a set of summary tables, this is the first paper to address maintaining summary tables efficiently.

Adaptive acoustic echo cancellation based on FIR and IIR filter banks
Null

Extrapolated impulse response filter and its application in the synthesis of digital filters using the frequency-response masking technique
Data warehouse refreshment is often viewed as a problem of maintaining materialized views over operational sources. In this paper, we show that the data warehouse refreshment process is a complex process comprising several tasks, e.g.,monitoring, extracting, transforming, integrating and cleaning operational data, deriving new data, building histories and loading the data warehouse. We propose a novel approach for defining and executing the refreshment process based on specifications stored in an object-oriented metadata repository. Our approach considers the multidimensional character of OLAP data and can be used in conjunction with various operational sources and target data warehouses.

Frequency resolution properties of the wavelet transform for detecting harmonically related narrowband signals
Data compression is an effective technique to improve the performance of data warehouses. Aggregation and cube are important operations for on-line analytical processing (OLAP). It is a major challenge to develop efficient algorithms for aggregation and cube operations on compressed data warehouses. Many efficient algorithms to compute aggregation and cube for relational OLAP have been developed. Some work has been done on efficiently computing aggregation and cube for multidimensional data warehouses (MDWs) that store datasets in multidimensional arrays rather than in tables. However, to our knowledge, there is few to date in the literature describing aggregation algorithms on compressed data warehouses for multidimensional OLAP.

Sinusoidal frequency estimation in chaotic noise
Null

Robust Frequency Domain Acoustic Echo Cancellation Filter Employing Normalized Residual Echo Enhancement
Three books that set the standard with their groundbreaking methods and techniques for data warehousing. Great value! Save $20, value of the set is $165. This set includes the following books from the Kimball Group: The Data Warehouse Toolkit, Second Edition This authoritative guide presents clear-cut guidelines for designing dimensional models through the use of real-world data warehouse case studies drawn from a variety of business application areas and industries. Ralph Kimball and Margy Ross help you master the full range of powerful techniques for designing dimensional databases that are easy to understand and provide fast query responses. The Data Warehouse Lifecycle Toolkit, Second Edition With this second edition, Ralph Kimball and his Kimball Group colleagues refined the original set of Lifecycle methods and techniques based on their consulting and training experiences. They walk you through detailed steps of designing, developing, and deploying a data warehousing/business intelligence system, from business requirements gathering through delivery, with the steadfast goal of enabling users to make better, more informed business decisions. The Data Warehouse ETL Toolkit Serving as a road map for planning, designing, building, and running the backroom of a data warehouse, this book provides complete coverage of proven, time-saving ETL techniques. You'll discover how a properly designed ETL system extracts data from the source systems, enforces data quality and consistency standards, conforms the data so that separate sources can be used together, and finally delivers the data in a presentation-ready format

A New Method for Designing Flat Shelving and Peaking Filters Based on Allpass Filters
Systems for managing and querying semistructured-data sources often store data in proprietary object repositories or in a tagged-text format. We describe a technique that can use relational database management systems to store and manage semistructured data. Our technique relies on a mapping between the semistructured data model and the relational data model, expressed in a query language called STORED. When a semistructured data instance is given, a STORED mapping can be generated automatically using data-mining techniques. We are interested in applying STORED to XML data, which is an instance of semistructured data. We show how a document-type-descriptor (DTD), when present, can be exploited to further improve performance.


Topic 14
Implementation and Effectiveness of a Feedback Feature in Underlining to Promote Reading Comprehension of e-Learning Instructional Materials
With increasing amount of data being stored in XML format, OLAP queries over these data become important. OLAP queries have been well studied in the relational database systems. However, the evaluation of OLAP queries over XML data is not a trivial extension of the relational solutions, especially when a schema is not available. In this paper, we introduce the IX-cube (Iceberg XML cube) over XML data to tackle the problem. We extend OLAP operations to XML data. We also develop efficient approaches to IX-Cube computation and OLAP query evaluation using IX-cubes.

Personalized E-learning system with self-regulated learning assisted mechanisms for promoting learning performance
Data warehouses using a multidimensional view of data have become very popular in both business and science in recent years. Data warehouses for scientific purposes such as medicine and bio-chemistry1 pose several great challenges to existing data warehouse technology. Data warehouses usually use pre-aggregated data to ensure fast query response. However, pre-aggregation cannot be used in practice if the dimension structures or the relationships between facts and dimensions are irregular. A technique for overcoming this limitation and some experimental results are presented. Queries over scientific data warehouses often need to reference data that is external to the data warehouse, e.g., data that is too complex to be handled by current data warehouse technology, data that is ”owned” by other organizations, or data that is updated frequently. This paper presents a federation architecture that allows the integration of multidimensional warehouse data with complex external data.

Why Is Externally-Regulated Learning More Effective Than Self-Regulated Learning with Hypermedia?
Many large organizations have developed data warehouses to support decision making. The data in a warehouse are subject oriented, integrated, time variant, and nonvolatile. A data warehouse contains five types of data: current detail data, older detail data, lightly summarized data, highly summarized data, and metadata. The architecture of a data warehouse includes a backend process (the extraction of data from source systems), the warehouse, and the front-end use (the accessing of data from the warehouse). A data mart is a smaller version of a data warehouse that supports the narrower set of requirements of a single business unit. Data marts should be developed in an integrated manner in order to avoid repeating the "silos of information" problem.An operational data store is a database for transaction processing systems that uses the data warehouse approach to provide clean data. Data warehousing is constantly changing, with the associated opportunities for practice and research, such as the potential for knowledge management using the warehouse.

E-studium: an Italian experience of blended e-learning E-studium: an Italian experience of blended e-learning
Data warehouses contain large amounts of information, often collected from a variety of independent sources. Decision-support functions in a warehouse, such as on-line analytical processing (OLAP), involve hundreds of complex aggregate queries over large volumes of data. It is not feasible to compute these queries by scanning the data sets each time. Warehouse applications therefore build a large number of summary tables, or materialized aggregate views, to help them increase the system performance. As changes, most notably new transactional data, are collected at the data sources, all summary tables at the warehouse that depend upon this data need to be updated. Usually, source changes are loaded into the warehouse at regular intervals, usually once a day, in a batch window, and the warehouse is made unavailable for querying while it is updated. Since the number of summary tables that need to be maintained is often large, a critical issue for data warehousing is how to maintain the summary tables efficiently. In this paper we propose a method of maintaining aggregate views (the summary-delta table method), and use it to solve two problems in maintaining summary tables in a warehouse: (1) how to efficiently maintain a summary table while minimizing the batch window needed for maintenance, and (2) how to maintain a large set of summary tables defined over the same base tables. While several papers have addressed the issues relating to choosing and materializing a set of summary tables, this is the first paper to address maintaining summary tables efficiently.

Scaffolding inquiry learning: How much intelligence is needed and by whom?
Null

Acculturated blended learning: localizing a blended learning course for Russian trainees
Data warehouse refreshment is often viewed as a problem of maintaining materialized views over operational sources. In this paper, we show that the data warehouse refreshment process is a complex process comprising several tasks, e.g.,monitoring, extracting, transforming, integrating and cleaning operational data, deriving new data, building histories and loading the data warehouse. We propose a novel approach for defining and executing the refreshment process based on specifications stored in an object-oriented metadata repository. Our approach considers the multidimensional character of OLAP data and can be used in conjunction with various operational sources and target data warehouses.

Do Various Self-Regulatory Processes Predict Different Hypermedia Learning Outcomes?
Data compression is an effective technique to improve the performance of data warehouses. Aggregation and cube are important operations for on-line analytical processing (OLAP). It is a major challenge to develop efficient algorithms for aggregation and cube operations on compressed data warehouses. Many efficient algorithms to compute aggregation and cube for relational OLAP have been developed. Some work has been done on efficiently computing aggregation and cube for multidimensional data warehouses (MDWs) that store datasets in multidimensional arrays rather than in tables. However, to our knowledge, there is few to date in the literature describing aggregation algorithms on compressed data warehouses for multidimensional OLAP.

Getting the Most from Online Learning: A Learner's Guide
Null

Using blended learning to enhance teaching and learning
Three books that set the standard with their groundbreaking methods and techniques for data warehousing. Great value! Save $20, value of the set is $165. This set includes the following books from the Kimball Group: The Data Warehouse Toolkit, Second Edition This authoritative guide presents clear-cut guidelines for designing dimensional models through the use of real-world data warehouse case studies drawn from a variety of business application areas and industries. Ralph Kimball and Margy Ross help you master the full range of powerful techniques for designing dimensional databases that are easy to understand and provide fast query responses. The Data Warehouse Lifecycle Toolkit, Second Edition With this second edition, Ralph Kimball and his Kimball Group colleagues refined the original set of Lifecycle methods and techniques based on their consulting and training experiences. They walk you through detailed steps of designing, developing, and deploying a data warehousing/business intelligence system, from business requirements gathering through delivery, with the steadfast goal of enabling users to make better, more informed business decisions. The Data Warehouse ETL Toolkit Serving as a road map for planning, designing, building, and running the backroom of a data warehouse, this book provides complete coverage of proven, time-saving ETL techniques. You'll discover how a properly designed ETL system extracts data from the source systems, enforces data quality and consistency standards, conforms the data so that separate sources can be used together, and finally delivers the data in a presentation-ready format

The Dynamic Nature of Self-Regulatory Behavior in Self-Regulated Learning and Externally-Regulated Learning Episodes
Systems for managing and querying semistructured-data sources often store data in proprietary object repositories or in a tagged-text format. We describe a technique that can use relational database management systems to store and manage semistructured data. Our technique relies on a mapping between the semistructured data model and the relational data model, expressed in a query language called STORED. When a semistructured data instance is given, a STORED mapping can be generated automatically using data-mining techniques. We are interested in applying STORED to XML data, which is an instance of semistructured data. We show how a document-type-descriptor (DTD), when present, can be exploited to further improve performance.


Topic 21
Letting software engineers do software engineering or freeing software engineers from the shackles of documentation
With increasing amount of data being stored in XML format, OLAP queries over these data become important. OLAP queries have been well studied in the relational database systems. However, the evaluation of OLAP queries over XML data is not a trivial extension of the relational solutions, especially when a schema is not available. In this paper, we introduce the IX-cube (Iceberg XML cube) over XML data to tackle the problem. We extend OLAP operations to XML data. We also develop efficient approaches to IX-Cube computation and OLAP query evaluation using IX-cubes.

Industrial Session - Introduction, Foundation, Information Engineering Workbench, Excelerator, ECASET, GENOS, PAPICS, ASA, LOGISCOPE, TASIS-CASE, Mega CASE Tools, The MastER System, SECSI, DAFNE Methodology, Software through Pictures, Olivetti Open Architecture, PREDICT CASE, easyFIND, ER-Modeler
Data warehouses using a multidimensional view of data have become very popular in both business and science in recent years. Data warehouses for scientific purposes such as medicine and bio-chemistry1 pose several great challenges to existing data warehouse technology. Data warehouses usually use pre-aggregated data to ensure fast query response. However, pre-aggregation cannot be used in practice if the dimension structures or the relationships between facts and dimensions are irregular. A technique for overcoming this limitation and some experimental results are presented. Queries over scientific data warehouses often need to reference data that is external to the data warehouse, e.g., data that is too complex to be handled by current data warehouse technology, data that is ”owned” by other organizations, or data that is updated frequently. This paper presents a federation architecture that allows the integration of multidimensional warehouse data with complex external data.

Maintenance in Joint Software Development
Many large organizations have developed data warehouses to support decision making. The data in a warehouse are subject oriented, integrated, time variant, and nonvolatile. A data warehouse contains five types of data: current detail data, older detail data, lightly summarized data, highly summarized data, and metadata. The architecture of a data warehouse includes a backend process (the extraction of data from source systems), the warehouse, and the front-end use (the accessing of data from the warehouse). A data mart is a smaller version of a data warehouse that supports the narrower set of requirements of a single business unit. Data marts should be developed in an integrated manner in order to avoid repeating the "silos of information" problem.An operational data store is a database for transaction processing systems that uses the data warehouse approach to provide clean data. Data warehousing is constantly changing, with the associated opportunities for practice and research, such as the potential for knowledge management using the warehouse.

TSPSM Coaching Development Teams (SEI Series in Software Engineering), by Watts S. Humphrey
Data warehouses contain large amounts of information, often collected from a variety of independent sources. Decision-support functions in a warehouse, such as on-line analytical processing (OLAP), involve hundreds of complex aggregate queries over large volumes of data. It is not feasible to compute these queries by scanning the data sets each time. Warehouse applications therefore build a large number of summary tables, or materialized aggregate views, to help them increase the system performance. As changes, most notably new transactional data, are collected at the data sources, all summary tables at the warehouse that depend upon this data need to be updated. Usually, source changes are loaded into the warehouse at regular intervals, usually once a day, in a batch window, and the warehouse is made unavailable for querying while it is updated. Since the number of summary tables that need to be maintained is often large, a critical issue for data warehousing is how to maintain the summary tables efficiently. In this paper we propose a method of maintaining aggregate views (the summary-delta table method), and use it to solve two problems in maintaining summary tables in a warehouse: (1) how to efficiently maintain a summary table while minimizing the batch window needed for maintenance, and (2) how to maintain a large set of summary tables defined over the same base tables. While several papers have addressed the issues relating to choosing and materializing a set of summary tables, this is the first paper to address maintaining summary tables efficiently.

Measuring Abstractness for Reverse Engineering in a Re-engineering Tool
Null

Experience Report - Restructure of Processes Based on ISO/IEC 12207 and SW-CMM in CELEPAR
Data warehouse refreshment is often viewed as a problem of maintaining materialized views over operational sources. In this paper, we show that the data warehouse refreshment process is a complex process comprising several tasks, e.g.,monitoring, extracting, transforming, integrating and cleaning operational data, deriving new data, building histories and loading the data warehouse. We propose a novel approach for defining and executing the refreshment process based on specifications stored in an object-oriented metadata repository. Our approach considers the multidimensional character of OLAP data and can be used in conjunction with various operational sources and target data warehouses.

MIL-STD 1803&mdash;Software development integrity program implementation&mdash;a cooperative approach to software development
Data compression is an effective technique to improve the performance of data warehouses. Aggregation and cube are important operations for on-line analytical processing (OLAP). It is a major challenge to develop efficient algorithms for aggregation and cube operations on compressed data warehouses. Many efficient algorithms to compute aggregation and cube for relational OLAP have been developed. Some work has been done on efficiently computing aggregation and cube for multidimensional data warehouses (MDWs) that store datasets in multidimensional arrays rather than in tables. However, to our knowledge, there is few to date in the literature describing aggregation algorithms on compressed data warehouses for multidimensional OLAP.

Using evolution patterns to find duplicated bugs
Null

A study of the effects of software development practices on software maintenance effort
Three books that set the standard with their groundbreaking methods and techniques for data warehousing. Great value! Save $20, value of the set is $165. This set includes the following books from the Kimball Group: The Data Warehouse Toolkit, Second Edition This authoritative guide presents clear-cut guidelines for designing dimensional models through the use of real-world data warehouse case studies drawn from a variety of business application areas and industries. Ralph Kimball and Margy Ross help you master the full range of powerful techniques for designing dimensional databases that are easy to understand and provide fast query responses. The Data Warehouse Lifecycle Toolkit, Second Edition With this second edition, Ralph Kimball and his Kimball Group colleagues refined the original set of Lifecycle methods and techniques based on their consulting and training experiences. They walk you through detailed steps of designing, developing, and deploying a data warehousing/business intelligence system, from business requirements gathering through delivery, with the steadfast goal of enabling users to make better, more informed business decisions. The Data Warehouse ETL Toolkit Serving as a road map for planning, designing, building, and running the backroom of a data warehouse, this book provides complete coverage of proven, time-saving ETL techniques. You'll discover how a properly designed ETL system extracts data from the source systems, enforces data quality and consistency standards, conforms the data so that separate sources can be used together, and finally delivers the data in a presentation-ready format

ToolCASE: a repository of computer-aided software engineering tools
Systems for managing and querying semistructured-data sources often store data in proprietary object repositories or in a tagged-text format. We describe a technique that can use relational database management systems to store and manage semistructured data. Our technique relies on a mapping between the semistructured data model and the relational data model, expressed in a query language called STORED. When a semistructured data instance is given, a STORED mapping can be generated automatically using data-mining techniques. We are interested in applying STORED to XML data, which is an instance of semistructured data. We show how a document-type-descriptor (DTD), when present, can be exploited to further improve performance.


Topic 22
Scheduling in dynamic assembly job-shops to minimize the sum of weighted earliness, weighted tardiness and weighted flowtime of jobs
With increasing amount of data being stored in XML format, OLAP queries over these data become important. OLAP queries have been well studied in the relational database systems. However, the evaluation of OLAP queries over XML data is not a trivial extension of the relational solutions, especially when a schema is not available. In this paper, we introduce the IX-cube (Iceberg XML cube) over XML data to tackle the problem. We extend OLAP operations to XML data. We also develop efficient approaches to IX-Cube computation and OLAP query evaluation using IX-cubes.

Slack Stealing Job Admission Control Scheduling
Data warehouses using a multidimensional view of data have become very popular in both business and science in recent years. Data warehouses for scientific purposes such as medicine and bio-chemistry1 pose several great challenges to existing data warehouse technology. Data warehouses usually use pre-aggregated data to ensure fast query response. However, pre-aggregation cannot be used in practice if the dimension structures or the relationships between facts and dimensions are irregular. A technique for overcoming this limitation and some experimental results are presented. Queries over scientific data warehouses often need to reference data that is external to the data warehouse, e.g., data that is too complex to be handled by current data warehouse technology, data that is ”owned” by other organizations, or data that is updated frequently. This paper presents a federation architecture that allows the integration of multidimensional warehouse data with complex external data.

Effects of nonsymmetric release times on rate monotonic scheduling
Many large organizations have developed data warehouses to support decision making. The data in a warehouse are subject oriented, integrated, time variant, and nonvolatile. A data warehouse contains five types of data: current detail data, older detail data, lightly summarized data, highly summarized data, and metadata. The architecture of a data warehouse includes a backend process (the extraction of data from source systems), the warehouse, and the front-end use (the accessing of data from the warehouse). A data mart is a smaller version of a data warehouse that supports the narrower set of requirements of a single business unit. Data marts should be developed in an integrated manner in order to avoid repeating the "silos of information" problem.An operational data store is a database for transaction processing systems that uses the data warehouse approach to provide clean data. Data warehousing is constantly changing, with the associated opportunities for practice and research, such as the potential for knowledge management using the warehouse.

A Reservation-Based Algorithm for Scheduling Both Periodic and Aperiodic Real-Time Tasks
Data warehouses contain large amounts of information, often collected from a variety of independent sources. Decision-support functions in a warehouse, such as on-line analytical processing (OLAP), involve hundreds of complex aggregate queries over large volumes of data. It is not feasible to compute these queries by scanning the data sets each time. Warehouse applications therefore build a large number of summary tables, or materialized aggregate views, to help them increase the system performance. As changes, most notably new transactional data, are collected at the data sources, all summary tables at the warehouse that depend upon this data need to be updated. Usually, source changes are loaded into the warehouse at regular intervals, usually once a day, in a batch window, and the warehouse is made unavailable for querying while it is updated. Since the number of summary tables that need to be maintained is often large, a critical issue for data warehousing is how to maintain the summary tables efficiently. In this paper we propose a method of maintaining aggregate views (the summary-delta table method), and use it to solve two problems in maintaining summary tables in a warehouse: (1) how to efficiently maintain a summary table while minimizing the batch window needed for maintenance, and (2) how to maintain a large set of summary tables defined over the same base tables. While several papers have addressed the issues relating to choosing and materializing a set of summary tables, this is the first paper to address maintaining summary tables efficiently.

Single Machine Batch Scheduling Problem with Resource Dependent Setup and Processing Time in the Presence of Fuzzy Due Date
Null

Fair Scheduling Algorithms in Grids
Data warehouse refreshment is often viewed as a problem of maintaining materialized views over operational sources. In this paper, we show that the data warehouse refreshment process is a complex process comprising several tasks, e.g.,monitoring, extracting, transforming, integrating and cleaning operational data, deriving new data, building histories and loading the data warehouse. We propose a novel approach for defining and executing the refreshment process based on specifications stored in an object-oriented metadata repository. Our approach considers the multidimensional character of OLAP data and can be used in conjunction with various operational sources and target data warehouses.

A Schedulable Utilization Bound for Aperiodic Tasks
Data compression is an effective technique to improve the performance of data warehouses. Aggregation and cube are important operations for on-line analytical processing (OLAP). It is a major challenge to develop efficient algorithms for aggregation and cube operations on compressed data warehouses. Many efficient algorithms to compute aggregation and cube for relational OLAP have been developed. Some work has been done on efficiently computing aggregation and cube for multidimensional data warehouses (MDWs) that store datasets in multidimensional arrays rather than in tables. However, to our knowledge, there is few to date in the literature describing aggregation algorithms on compressed data warehouses for multidimensional OLAP.

Minimizing weighted number of tardy jobs and weighted earliness-tardiness penalties about a common due date
Null

Integrating Priority with Share in the Priority-Based Weighted Fair Queuing Scheduler for Real-Time Networks
Three books that set the standard with their groundbreaking methods and techniques for data warehousing. Great value! Save $20, value of the set is $165. This set includes the following books from the Kimball Group: The Data Warehouse Toolkit, Second Edition This authoritative guide presents clear-cut guidelines for designing dimensional models through the use of real-world data warehouse case studies drawn from a variety of business application areas and industries. Ralph Kimball and Margy Ross help you master the full range of powerful techniques for designing dimensional databases that are easy to understand and provide fast query responses. The Data Warehouse Lifecycle Toolkit, Second Edition With this second edition, Ralph Kimball and his Kimball Group colleagues refined the original set of Lifecycle methods and techniques based on their consulting and training experiences. They walk you through detailed steps of designing, developing, and deploying a data warehousing/business intelligence system, from business requirements gathering through delivery, with the steadfast goal of enabling users to make better, more informed business decisions. The Data Warehouse ETL Toolkit Serving as a road map for planning, designing, building, and running the backroom of a data warehouse, this book provides complete coverage of proven, time-saving ETL techniques. You'll discover how a properly designed ETL system extracts data from the source systems, enforces data quality and consistency standards, conforms the data so that separate sources can be used together, and finally delivers the data in a presentation-ready format

Scheduling periodic and aperiodic tasks in hard real-time computing systems
Systems for managing and querying semistructured-data sources often store data in proprietary object repositories or in a tagged-text format. We describe a technique that can use relational database management systems to store and manage semistructured data. Our technique relies on a mapping between the semistructured data model and the relational data model, expressed in a query language called STORED. When a semistructured data instance is given, a STORED mapping can be generated automatically using data-mining techniques. We are interested in applying STORED to XML data, which is an instance of semistructured data. We show how a document-type-descriptor (DTD), when present, can be exploited to further improve performance.


Topic 29
Hybrid diversity combing techniques for DS-CDMA over a multipath fading channel
With increasing amount of data being stored in XML format, OLAP queries over these data become important. OLAP queries have been well studied in the relational database systems. However, the evaluation of OLAP queries over XML data is not a trivial extension of the relational solutions, especially when a schema is not available. In this paper, we introduce the IX-cube (Iceberg XML cube) over XML data to tackle the problem. We extend OLAP operations to XML data. We also develop efficient approaches to IX-Cube computation and OLAP query evaluation using IX-cubes.

Performance of Super-Orthogonal Convolutional Coding for Ultra-Wideband Systems in Multipath and Multiuser Channels
Data warehouses using a multidimensional view of data have become very popular in both business and science in recent years. Data warehouses for scientific purposes such as medicine and bio-chemistry1 pose several great challenges to existing data warehouse technology. Data warehouses usually use pre-aggregated data to ensure fast query response. However, pre-aggregation cannot be used in practice if the dimension structures or the relationships between facts and dimensions are irregular. A technique for overcoming this limitation and some experimental results are presented. Queries over scientific data warehouses often need to reference data that is external to the data warehouse, e.g., data that is too complex to be handled by current data warehouse technology, data that is ”owned” by other organizations, or data that is updated frequently. This paper presents a federation architecture that allows the integration of multidimensional warehouse data with complex external data.

Comparison of TCM and BICM in wireless LAN system over indoor wireless channel
Many large organizations have developed data warehouses to support decision making. The data in a warehouse are subject oriented, integrated, time variant, and nonvolatile. A data warehouse contains five types of data: current detail data, older detail data, lightly summarized data, highly summarized data, and metadata. The architecture of a data warehouse includes a backend process (the extraction of data from source systems), the warehouse, and the front-end use (the accessing of data from the warehouse). A data mart is a smaller version of a data warehouse that supports the narrower set of requirements of a single business unit. Data marts should be developed in an integrated manner in order to avoid repeating the "silos of information" problem.An operational data store is a database for transaction processing systems that uses the data warehouse approach to provide clean data. Data warehousing is constantly changing, with the associated opportunities for practice and research, such as the potential for knowledge management using the warehouse.

Outage Performance and Average Symbol Error Rate of M-QAM for Maximum Ratio Combining with Multiple Interferers
Data warehouses contain large amounts of information, often collected from a variety of independent sources. Decision-support functions in a warehouse, such as on-line analytical processing (OLAP), involve hundreds of complex aggregate queries over large volumes of data. It is not feasible to compute these queries by scanning the data sets each time. Warehouse applications therefore build a large number of summary tables, or materialized aggregate views, to help them increase the system performance. As changes, most notably new transactional data, are collected at the data sources, all summary tables at the warehouse that depend upon this data need to be updated. Usually, source changes are loaded into the warehouse at regular intervals, usually once a day, in a batch window, and the warehouse is made unavailable for querying while it is updated. Since the number of summary tables that need to be maintained is often large, a critical issue for data warehousing is how to maintain the summary tables efficiently. In this paper we propose a method of maintaining aggregate views (the summary-delta table method), and use it to solve two problems in maintaining summary tables in a warehouse: (1) how to efficiently maintain a summary table while minimizing the batch window needed for maintenance, and (2) how to maintain a large set of summary tables defined over the same base tables. While several papers have addressed the issues relating to choosing and materializing a set of summary tables, this is the first paper to address maintaining summary tables efficiently.

An exact error probability analysis of OFDM systems with frequency offset
Null

Design of optimal soft decoding for combined trellis coded quantization/modulation in Rayleigh fading channel
Data warehouse refreshment is often viewed as a problem of maintaining materialized views over operational sources. In this paper, we show that the data warehouse refreshment process is a complex process comprising several tasks, e.g.,monitoring, extracting, transforming, integrating and cleaning operational data, deriving new data, building histories and loading the data warehouse. We propose a novel approach for defining and executing the refreshment process based on specifications stored in an object-oriented metadata repository. Our approach considers the multidimensional character of OLAP data and can be used in conjunction with various operational sources and target data warehouses.

Cyclic delay diversity with frequency domain turbo equalization for uplink fast fading channels
Data compression is an effective technique to improve the performance of data warehouses. Aggregation and cube are important operations for on-line analytical processing (OLAP). It is a major challenge to develop efficient algorithms for aggregation and cube operations on compressed data warehouses. Many efficient algorithms to compute aggregation and cube for relational OLAP have been developed. Some work has been done on efficiently computing aggregation and cube for multidimensional data warehouses (MDWs) that store datasets in multidimensional arrays rather than in tables. However, to our knowledge, there is few to date in the literature describing aggregation algorithms on compressed data warehouses for multidimensional OLAP.

Iterative MAP equalization and decoding in wireless mobile coded OFDM
Null

ZCZ-CDMA and OFDMA using M-QAM for broadband wireless communications: Research Articles
Three books that set the standard with their groundbreaking methods and techniques for data warehousing. Great value! Save $20, value of the set is $165. This set includes the following books from the Kimball Group: The Data Warehouse Toolkit, Second Edition This authoritative guide presents clear-cut guidelines for designing dimensional models through the use of real-world data warehouse case studies drawn from a variety of business application areas and industries. Ralph Kimball and Margy Ross help you master the full range of powerful techniques for designing dimensional databases that are easy to understand and provide fast query responses. The Data Warehouse Lifecycle Toolkit, Second Edition With this second edition, Ralph Kimball and his Kimball Group colleagues refined the original set of Lifecycle methods and techniques based on their consulting and training experiences. They walk you through detailed steps of designing, developing, and deploying a data warehousing/business intelligence system, from business requirements gathering through delivery, with the steadfast goal of enabling users to make better, more informed business decisions. The Data Warehouse ETL Toolkit Serving as a road map for planning, designing, building, and running the backroom of a data warehouse, this book provides complete coverage of proven, time-saving ETL techniques. You'll discover how a properly designed ETL system extracts data from the source systems, enforces data quality and consistency standards, conforms the data so that separate sources can be used together, and finally delivers the data in a presentation-ready format

On the Performance of V-BLAST Detectors in MIMO MT-CDMA Uplink over Multipath Fading Channel
Systems for managing and querying semistructured-data sources often store data in proprietary object repositories or in a tagged-text format. We describe a technique that can use relational database management systems to store and manage semistructured data. Our technique relies on a mapping between the semistructured data model and the relational data model, expressed in a query language called STORED. When a semistructured data instance is given, a STORED mapping can be generated automatically using data-mining techniques. We are interested in applying STORED to XML data, which is an instance of semistructured data. We show how a document-type-descriptor (DTD), when present, can be exploited to further improve performance.


Topic 37
ANS MUMPS programmer's reference manual, 1981 and ANS MUMPS language standard: revision of the MUMPS programmer's reference manual
With increasing amount of data being stored in XML format, OLAP queries over these data become important. OLAP queries have been well studied in the relational database systems. However, the evaluation of OLAP queries over XML data is not a trivial extension of the relational solutions, especially when a schema is not available. In this paper, we introduce the IX-cube (Iceberg XML cube) over XML data to tackle the problem. We extend OLAP operations to XML data. We also develop efficient approaches to IX-Cube computation and OLAP query evaluation using IX-cubes.

Write TSRs Now with Borland's Turbo Assembler, Turbo C/C++, Turbo Pascal
Data warehouses using a multidimensional view of data have become very popular in both business and science in recent years. Data warehouses for scientific purposes such as medicine and bio-chemistry1 pose several great challenges to existing data warehouse technology. Data warehouses usually use pre-aggregated data to ensure fast query response. However, pre-aggregation cannot be used in practice if the dimension structures or the relationships between facts and dimensions are irregular. A technique for overcoming this limitation and some experimental results are presented. Queries over scientific data warehouses often need to reference data that is external to the data warehouse, e.g., data that is too complex to be handled by current data warehouse technology, data that is ”owned” by other organizations, or data that is updated frequently. This paper presents a federation architecture that allows the integration of multidimensional warehouse data with complex external data.

Python/C API Manual - PYTHON 2.6: Python documentation MANUAL Part 4
Many large organizations have developed data warehouses to support decision making. The data in a warehouse are subject oriented, integrated, time variant, and nonvolatile. A data warehouse contains five types of data: current detail data, older detail data, lightly summarized data, highly summarized data, and metadata. The architecture of a data warehouse includes a backend process (the extraction of data from source systems), the warehouse, and the front-end use (the accessing of data from the warehouse). A data mart is a smaller version of a data warehouse that supports the narrower set of requirements of a single business unit. Data marts should be developed in an integrated manner in order to avoid repeating the "silos of information" problem.An operational data store is a database for transaction processing systems that uses the data warehouse approach to provide clean data. Data warehousing is constantly changing, with the associated opportunities for practice and research, such as the potential for knowledge management using the warehouse.

The annotated ANSI C Standard American National Standard for Programming Languages&mdash;C: ANSI/ISO 9899-1990
Data warehouses contain large amounts of information, often collected from a variety of independent sources. Decision-support functions in a warehouse, such as on-line analytical processing (OLAP), involve hundreds of complex aggregate queries over large volumes of data. It is not feasible to compute these queries by scanning the data sets each time. Warehouse applications therefore build a large number of summary tables, or materialized aggregate views, to help them increase the system performance. As changes, most notably new transactional data, are collected at the data sources, all summary tables at the warehouse that depend upon this data need to be updated. Usually, source changes are loaded into the warehouse at regular intervals, usually once a day, in a batch window, and the warehouse is made unavailable for querying while it is updated. Since the number of summary tables that need to be maintained is often large, a critical issue for data warehousing is how to maintain the summary tables efficiently. In this paper we propose a method of maintaining aggregate views (the summary-delta table method), and use it to solve two problems in maintaining summary tables in a warehouse: (1) how to efficiently maintain a summary table while minimizing the batch window needed for maintenance, and (2) how to maintain a large set of summary tables defined over the same base tables. While several papers have addressed the issues relating to choosing and materializing a set of summary tables, this is the first paper to address maintaining summary tables efficiently.

PYTHON 2.6 and C or C++: Extending and Embedding Python
Null

Book review: Introduction to Common Lisp by Taiichi Yuasa and Masami Hagiya (Academic Press, 1987) and Common Lisp Drill by Taiichi Yuasa (Academic Press, 1988); Both texts translated into English by Richard Weyhrauch and Yasuko Kitajima
Data warehouse refreshment is often viewed as a problem of maintaining materialized views over operational sources. In this paper, we show that the data warehouse refreshment process is a complex process comprising several tasks, e.g.,monitoring, extracting, transforming, integrating and cleaning operational data, deriving new data, building histories and loading the data warehouse. We propose a novel approach for defining and executing the refreshment process based on specifications stored in an object-oriented metadata repository. Our approach considers the multidimensional character of OLAP data and can be used in conjunction with various operational sources and target data warehouses.

The complete MUMPS: an introduction and reference manual for the MUMPS programming language
Data compression is an effective technique to improve the performance of data warehouses. Aggregation and cube are important operations for on-line analytical processing (OLAP). It is a major challenge to develop efficient algorithms for aggregation and cube operations on compressed data warehouses. Many efficient algorithms to compute aggregation and cube for relational OLAP have been developed. Some work has been done on efficiently computing aggregation and cube for multidimensional data warehouses (MDWs) that store datasets in multidimensional arrays rather than in tables. However, to our knowledge, there is few to date in the literature describing aggregation algorithms on compressed data warehouses for multidimensional OLAP.

Programming C++ with panache: a C++ style manual
Null

C: A Reference Manual, 5th edition
Three books that set the standard with their groundbreaking methods and techniques for data warehousing. Great value! Save $20, value of the set is $165. This set includes the following books from the Kimball Group: The Data Warehouse Toolkit, Second Edition This authoritative guide presents clear-cut guidelines for designing dimensional models through the use of real-world data warehouse case studies drawn from a variety of business application areas and industries. Ralph Kimball and Margy Ross help you master the full range of powerful techniques for designing dimensional databases that are easy to understand and provide fast query responses. The Data Warehouse Lifecycle Toolkit, Second Edition With this second edition, Ralph Kimball and his Kimball Group colleagues refined the original set of Lifecycle methods and techniques based on their consulting and training experiences. They walk you through detailed steps of designing, developing, and deploying a data warehousing/business intelligence system, from business requirements gathering through delivery, with the steadfast goal of enabling users to make better, more informed business decisions. The Data Warehouse ETL Toolkit Serving as a road map for planning, designing, building, and running the backroom of a data warehouse, this book provides complete coverage of proven, time-saving ETL techniques. You'll discover how a properly designed ETL system extracts data from the source systems, enforces data quality and consistency standards, conforms the data so that separate sources can be used together, and finally delivers the data in a presentation-ready format

Mastering C++: an introduction to C++ and object-oriented programming for C and Pascal programmers
Systems for managing and querying semistructured-data sources often store data in proprietary object repositories or in a tagged-text format. We describe a technique that can use relational database management systems to store and manage semistructured data. Our technique relies on a mapping between the semistructured data model and the relational data model, expressed in a query language called STORED. When a semistructured data instance is given, a STORED mapping can be generated automatically using data-mining techniques. We are interested in applying STORED to XML data, which is an instance of semistructured data. We show how a document-type-descriptor (DTD), when present, can be exploited to further improve performance.


Topic 41
Simple sequence repeats in organellar genomes of rice: frequency and distribution in genic and intergenic regions
With increasing amount of data being stored in XML format, OLAP queries over these data become important. OLAP queries have been well studied in the relational database systems. However, the evaluation of OLAP queries over XML data is not a trivial extension of the relational solutions, especially when a schema is not available. In this paper, we introduce the IX-cube (Iceberg XML cube) over XML data to tackle the problem. We extend OLAP operations to XML data. We also develop efficient approaches to IX-Cube computation and OLAP query evaluation using IX-cubes.

The Korea Brassica Genome Project: A glimpse of the Brassica genome based on comparative genome analysis with Arabidopsis: Conference Papers
Data warehouses using a multidimensional view of data have become very popular in both business and science in recent years. Data warehouses for scientific purposes such as medicine and bio-chemistry1 pose several great challenges to existing data warehouse technology. Data warehouses usually use pre-aggregated data to ensure fast query response. However, pre-aggregation cannot be used in practice if the dimension structures or the relationships between facts and dimensions are irregular. A technique for overcoming this limitation and some experimental results are presented. Queries over scientific data warehouses often need to reference data that is external to the data warehouse, e.g., data that is too complex to be handled by current data warehouse technology, data that is ”owned” by other organizations, or data that is updated frequently. This paper presents a federation architecture that allows the integration of multidimensional warehouse data with complex external data.

In silico analysis of motifs in promoters of Differentially Expressed Genes in rice (Oryza sativa L.) under anoxia
Many large organizations have developed data warehouses to support decision making. The data in a warehouse are subject oriented, integrated, time variant, and nonvolatile. A data warehouse contains five types of data: current detail data, older detail data, lightly summarized data, highly summarized data, and metadata. The architecture of a data warehouse includes a backend process (the extraction of data from source systems), the warehouse, and the front-end use (the accessing of data from the warehouse). A data mart is a smaller version of a data warehouse that supports the narrower set of requirements of a single business unit. Data marts should be developed in an integrated manner in order to avoid repeating the "silos of information" problem.An operational data store is a database for transaction processing systems that uses the data warehouse approach to provide clean data. Data warehousing is constantly changing, with the associated opportunities for practice and research, such as the potential for knowledge management using the warehouse.

PhosphoPOINT
Data warehouses contain large amounts of information, often collected from a variety of independent sources. Decision-support functions in a warehouse, such as on-line analytical processing (OLAP), involve hundreds of complex aggregate queries over large volumes of data. It is not feasible to compute these queries by scanning the data sets each time. Warehouse applications therefore build a large number of summary tables, or materialized aggregate views, to help them increase the system performance. As changes, most notably new transactional data, are collected at the data sources, all summary tables at the warehouse that depend upon this data need to be updated. Usually, source changes are loaded into the warehouse at regular intervals, usually once a day, in a batch window, and the warehouse is made unavailable for querying while it is updated. Since the number of summary tables that need to be maintained is often large, a critical issue for data warehousing is how to maintain the summary tables efficiently. In this paper we propose a method of maintaining aggregate views (the summary-delta table method), and use it to solve two problems in maintaining summary tables in a warehouse: (1) how to efficiently maintain a summary table while minimizing the batch window needed for maintenance, and (2) how to maintain a large set of summary tables defined over the same base tables. While several papers have addressed the issues relating to choosing and materializing a set of summary tables, this is the first paper to address maintaining summary tables efficiently.

Identification and analysis of novel tandem repeats in the cell surface proteins of archaeal and bacterial genomes using computational tools: Primary Research Papers
Null

Hemoglobin-binding protein HgbA in the outer membrane of Actinobacillus pleuropneumoniae: homology modelling reveals regions of potential interactions with hemoglobin and heme
Data warehouse refreshment is often viewed as a problem of maintaining materialized views over operational sources. In this paper, we show that the data warehouse refreshment process is a complex process comprising several tasks, e.g.,monitoring, extracting, transforming, integrating and cleaning operational data, deriving new data, building histories and loading the data warehouse. We propose a novel approach for defining and executing the refreshment process based on specifications stored in an object-oriented metadata repository. Our approach considers the multidimensional character of OLAP data and can be used in conjunction with various operational sources and target data warehouses.

Genome-wide analysis of intergenic regions of mycobacterium tuberculosis H37Rv using affymetrix genechips
Data compression is an effective technique to improve the performance of data warehouses. Aggregation and cube are important operations for on-line analytical processing (OLAP). It is a major challenge to develop efficient algorithms for aggregation and cube operations on compressed data warehouses. Many efficient algorithms to compute aggregation and cube for relational OLAP have been developed. Some work has been done on efficiently computing aggregation and cube for multidimensional data warehouses (MDWs) that store datasets in multidimensional arrays rather than in tables. However, to our knowledge, there is few to date in the literature describing aggregation algorithms on compressed data warehouses for multidimensional OLAP.

Genome Regions Involved in Multiple Regulatory Pathways Identified Using GSEL, A Genome-Wide Database of Regulatory Sequence Elements of Geobacter sulfurreducens
Null

Identification of Proteins from Tuberculin Purified Protein Derivative (PPD) with Potential for TB Diagnosis Using Bioinformatics Analysis
Three books that set the standard with their groundbreaking methods and techniques for data warehousing. Great value! Save $20, value of the set is $165. This set includes the following books from the Kimball Group: The Data Warehouse Toolkit, Second Edition This authoritative guide presents clear-cut guidelines for designing dimensional models through the use of real-world data warehouse case studies drawn from a variety of business application areas and industries. Ralph Kimball and Margy Ross help you master the full range of powerful techniques for designing dimensional databases that are easy to understand and provide fast query responses. The Data Warehouse Lifecycle Toolkit, Second Edition With this second edition, Ralph Kimball and his Kimball Group colleagues refined the original set of Lifecycle methods and techniques based on their consulting and training experiences. They walk you through detailed steps of designing, developing, and deploying a data warehousing/business intelligence system, from business requirements gathering through delivery, with the steadfast goal of enabling users to make better, more informed business decisions. The Data Warehouse ETL Toolkit Serving as a road map for planning, designing, building, and running the backroom of a data warehouse, this book provides complete coverage of proven, time-saving ETL techniques. You'll discover how a properly designed ETL system extracts data from the source systems, enforces data quality and consistency standards, conforms the data so that separate sources can be used together, and finally delivers the data in a presentation-ready format

Sequence analysis of GerM and SpoVS, uncharacterized bacterial ‘sporulation’ proteins with widespread phylogenetic distribution
Systems for managing and querying semistructured-data sources often store data in proprietary object repositories or in a tagged-text format. We describe a technique that can use relational database management systems to store and manage semistructured data. Our technique relies on a mapping between the semistructured data model and the relational data model, expressed in a query language called STORED. When a semistructured data instance is given, a STORED mapping can be generated automatically using data-mining techniques. We are interested in applying STORED to XML data, which is an instance of semistructured data. We show how a document-type-descriptor (DTD), when present, can be exploited to further improve performance.


Topic 43
Prediction of workability of concrete incorporating metakaolin and PFA using neural networks
With increasing amount of data being stored in XML format, OLAP queries over these data become important. OLAP queries have been well studied in the relational database systems. However, the evaluation of OLAP queries over XML data is not a trivial extension of the relational solutions, especially when a schema is not available. In this paper, we introduce the IX-cube (Iceberg XML cube) over XML data to tackle the problem. We extend OLAP operations to XML data. We also develop efficient approaches to IX-Cube computation and OLAP query evaluation using IX-cubes.

Prediction on Tribology Behavior of PEEK Composites Using Back Propagation Artificial Neural Networks
Data warehouses using a multidimensional view of data have become very popular in both business and science in recent years. Data warehouses for scientific purposes such as medicine and bio-chemistry1 pose several great challenges to existing data warehouse technology. Data warehouses usually use pre-aggregated data to ensure fast query response. However, pre-aggregation cannot be used in practice if the dimension structures or the relationships between facts and dimensions are irregular. A technique for overcoming this limitation and some experimental results are presented. Queries over scientific data warehouses often need to reference data that is external to the data warehouse, e.g., data that is too complex to be handled by current data warehouse technology, data that is ”owned” by other organizations, or data that is updated frequently. This paper presents a federation architecture that allows the integration of multidimensional warehouse data with complex external data.

Prediction of Reservior Runoff Using RBF Neural Network-Grey System United Model
Many large organizations have developed data warehouses to support decision making. The data in a warehouse are subject oriented, integrated, time variant, and nonvolatile. A data warehouse contains five types of data: current detail data, older detail data, lightly summarized data, highly summarized data, and metadata. The architecture of a data warehouse includes a backend process (the extraction of data from source systems), the warehouse, and the front-end use (the accessing of data from the warehouse). A data mart is a smaller version of a data warehouse that supports the narrower set of requirements of a single business unit. Data marts should be developed in an integrated manner in order to avoid repeating the "silos of information" problem.An operational data store is a database for transaction processing systems that uses the data warehouse approach to provide clean data. Data warehousing is constantly changing, with the associated opportunities for practice and research, such as the potential for knowledge management using the warehouse.

Design of a Power System Stabilizer Using a new Recurrent Neural Network
Data warehouses contain large amounts of information, often collected from a variety of independent sources. Decision-support functions in a warehouse, such as on-line analytical processing (OLAP), involve hundreds of complex aggregate queries over large volumes of data. It is not feasible to compute these queries by scanning the data sets each time. Warehouse applications therefore build a large number of summary tables, or materialized aggregate views, to help them increase the system performance. As changes, most notably new transactional data, are collected at the data sources, all summary tables at the warehouse that depend upon this data need to be updated. Usually, source changes are loaded into the warehouse at regular intervals, usually once a day, in a batch window, and the warehouse is made unavailable for querying while it is updated. Since the number of summary tables that need to be maintained is often large, a critical issue for data warehousing is how to maintain the summary tables efficiently. In this paper we propose a method of maintaining aggregate views (the summary-delta table method), and use it to solve two problems in maintaining summary tables in a warehouse: (1) how to efficiently maintain a summary table while minimizing the batch window needed for maintenance, and (2) how to maintain a large set of summary tables defined over the same base tables. While several papers have addressed the issues relating to choosing and materializing a set of summary tables, this is the first paper to address maintaining summary tables efficiently.

Centroid based Multilayer Perceptron Networks
Null

Performance of generalized multilayered perceptons trained using the Levenberg-Marquardt method
Data warehouse refreshment is often viewed as a problem of maintaining materialized views over operational sources. In this paper, we show that the data warehouse refreshment process is a complex process comprising several tasks, e.g.,monitoring, extracting, transforming, integrating and cleaning operational data, deriving new data, building histories and loading the data warehouse. We propose a novel approach for defining and executing the refreshment process based on specifications stored in an object-oriented metadata repository. Our approach considers the multidimensional character of OLAP data and can be used in conjunction with various operational sources and target data warehouses.

Radial Basis Neural Network Learning Based on Particle Swarm Optimization to Multistep Prediction of Chaotic Lorenz's System
Data compression is an effective technique to improve the performance of data warehouses. Aggregation and cube are important operations for on-line analytical processing (OLAP). It is a major challenge to develop efficient algorithms for aggregation and cube operations on compressed data warehouses. Many efficient algorithms to compute aggregation and cube for relational OLAP have been developed. Some work has been done on efficiently computing aggregation and cube for multidimensional data warehouses (MDWs) that store datasets in multidimensional arrays rather than in tables. However, to our knowledge, there is few to date in the literature describing aggregation algorithms on compressed data warehouses for multidimensional OLAP.

Nonlinear Pattern Identification by Multi-layered GMDH-Type Neural Network Self-selecting Optimum Neural Network Architecture
Null

Bernoulli Neural Network with Weights Directly Determined and with the Number of Hidden- Layer Neurons Automatically Determined
Three books that set the standard with their groundbreaking methods and techniques for data warehousing. Great value! Save $20, value of the set is $165. This set includes the following books from the Kimball Group: The Data Warehouse Toolkit, Second Edition This authoritative guide presents clear-cut guidelines for designing dimensional models through the use of real-world data warehouse case studies drawn from a variety of business application areas and industries. Ralph Kimball and Margy Ross help you master the full range of powerful techniques for designing dimensional databases that are easy to understand and provide fast query responses. The Data Warehouse Lifecycle Toolkit, Second Edition With this second edition, Ralph Kimball and his Kimball Group colleagues refined the original set of Lifecycle methods and techniques based on their consulting and training experiences. They walk you through detailed steps of designing, developing, and deploying a data warehousing/business intelligence system, from business requirements gathering through delivery, with the steadfast goal of enabling users to make better, more informed business decisions. The Data Warehouse ETL Toolkit Serving as a road map for planning, designing, building, and running the backroom of a data warehouse, this book provides complete coverage of proven, time-saving ETL techniques. You'll discover how a properly designed ETL system extracts data from the source systems, enforces data quality and consistency standards, conforms the data so that separate sources can be used together, and finally delivers the data in a presentation-ready format

A Modified Particle Swarm Optimization and Radial Basis Function Neural Network Hybrid Algorithm Model and Its Application
Systems for managing and querying semistructured-data sources often store data in proprietary object repositories or in a tagged-text format. We describe a technique that can use relational database management systems to store and manage semistructured data. Our technique relies on a mapping between the semistructured data model and the relational data model, expressed in a query language called STORED. When a semistructured data instance is given, a STORED mapping can be generated automatically using data-mining techniques. We are interested in applying STORED to XML data, which is an instance of semistructured data. We show how a document-type-descriptor (DTD), when present, can be exploited to further improve performance.


Topic 52
Equilibrium Price Communication and Unadvertised Specials by Competing Supermarkets
With increasing amount of data being stored in XML format, OLAP queries over these data become important. OLAP queries have been well studied in the relational database systems. However, the evaluation of OLAP queries over XML data is not a trivial extension of the relational solutions, especially when a schema is not available. In this paper, we introduce the IX-cube (Iceberg XML cube) over XML data to tackle the problem. We extend OLAP operations to XML data. We also develop efficient approaches to IX-Cube computation and OLAP query evaluation using IX-cubes.

The Impact of E-Commerce Announcements on the Market Value of Firms
Data warehouses using a multidimensional view of data have become very popular in both business and science in recent years. Data warehouses for scientific purposes such as medicine and bio-chemistry1 pose several great challenges to existing data warehouse technology. Data warehouses usually use pre-aggregated data to ensure fast query response. However, pre-aggregation cannot be used in practice if the dimension structures or the relationships between facts and dimensions are irregular. A technique for overcoming this limitation and some experimental results are presented. Queries over scientific data warehouses often need to reference data that is external to the data warehouse, e.g., data that is too complex to be handled by current data warehouse technology, data that is ”owned” by other organizations, or data that is updated frequently. This paper presents a federation architecture that allows the integration of multidimensional warehouse data with complex external data.

Competing sellers in online markets: reserve prices, shill bidding, and auction fees
Many large organizations have developed data warehouses to support decision making. The data in a warehouse are subject oriented, integrated, time variant, and nonvolatile. A data warehouse contains five types of data: current detail data, older detail data, lightly summarized data, highly summarized data, and metadata. The architecture of a data warehouse includes a backend process (the extraction of data from source systems), the warehouse, and the front-end use (the accessing of data from the warehouse). A data mart is a smaller version of a data warehouse that supports the narrower set of requirements of a single business unit. Data marts should be developed in an integrated manner in order to avoid repeating the "silos of information" problem.An operational data store is a database for transaction processing systems that uses the data warehouse approach to provide clean data. Data warehousing is constantly changing, with the associated opportunities for practice and research, such as the potential for knowledge management using the warehouse.

Individual Marketing with Imperfect Targetability
Data warehouses contain large amounts of information, often collected from a variety of independent sources. Decision-support functions in a warehouse, such as on-line analytical processing (OLAP), involve hundreds of complex aggregate queries over large volumes of data. It is not feasible to compute these queries by scanning the data sets each time. Warehouse applications therefore build a large number of summary tables, or materialized aggregate views, to help them increase the system performance. As changes, most notably new transactional data, are collected at the data sources, all summary tables at the warehouse that depend upon this data need to be updated. Usually, source changes are loaded into the warehouse at regular intervals, usually once a day, in a batch window, and the warehouse is made unavailable for querying while it is updated. Since the number of summary tables that need to be maintained is often large, a critical issue for data warehousing is how to maintain the summary tables efficiently. In this paper we propose a method of maintaining aggregate views (the summary-delta table method), and use it to solve two problems in maintaining summary tables in a warehouse: (1) how to efficiently maintain a summary table while minimizing the batch window needed for maintenance, and (2) how to maintain a large set of summary tables defined over the same base tables. While several papers have addressed the issues relating to choosing and materializing a set of summary tables, this is the first paper to address maintaining summary tables efficiently.

Supplier Empowerment
Null

Plunkett's E-Commerce & Internet Business Almanac 2007: E-Commerce & Internet Business Market Research, Statistics, Trends & Leading Companies (Plunkett's E-Commerce and Internet Business Almanac)
Data warehouse refreshment is often viewed as a problem of maintaining materialized views over operational sources. In this paper, we show that the data warehouse refreshment process is a complex process comprising several tasks, e.g.,monitoring, extracting, transforming, integrating and cleaning operational data, deriving new data, building histories and loading the data warehouse. We propose a novel approach for defining and executing the refreshment process based on specifications stored in an object-oriented metadata repository. Our approach considers the multidimensional character of OLAP data and can be used in conjunction with various operational sources and target data warehouses.

Consumer Addressability and Customized Pricing
Data compression is an effective technique to improve the performance of data warehouses. Aggregation and cube are important operations for on-line analytical processing (OLAP). It is a major challenge to develop efficient algorithms for aggregation and cube operations on compressed data warehouses. Many efficient algorithms to compute aggregation and cube for relational OLAP have been developed. Some work has been done on efficiently computing aggregation and cube for multidimensional data warehouses (MDWs) that store datasets in multidimensional arrays rather than in tables. However, to our knowledge, there is few to date in the literature describing aggregation algorithms on compressed data warehouses for multidimensional OLAP.

Multiple Messages to Retain Retailers: Signaling New Product Demand
Null

Delegating Pricing Decisions
Three books that set the standard with their groundbreaking methods and techniques for data warehousing. Great value! Save $20, value of the set is $165. This set includes the following books from the Kimball Group: The Data Warehouse Toolkit, Second Edition This authoritative guide presents clear-cut guidelines for designing dimensional models through the use of real-world data warehouse case studies drawn from a variety of business application areas and industries. Ralph Kimball and Margy Ross help you master the full range of powerful techniques for designing dimensional databases that are easy to understand and provide fast query responses. The Data Warehouse Lifecycle Toolkit, Second Edition With this second edition, Ralph Kimball and his Kimball Group colleagues refined the original set of Lifecycle methods and techniques based on their consulting and training experiences. They walk you through detailed steps of designing, developing, and deploying a data warehousing/business intelligence system, from business requirements gathering through delivery, with the steadfast goal of enabling users to make better, more informed business decisions. The Data Warehouse ETL Toolkit Serving as a road map for planning, designing, building, and running the backroom of a data warehouse, this book provides complete coverage of proven, time-saving ETL techniques. You'll discover how a properly designed ETL system extracts data from the source systems, enforces data quality and consistency standards, conforms the data so that separate sources can be used together, and finally delivers the data in a presentation-ready format

Electronic Tickets, Smart Cards, and Online Prepayments: When and How to Advance Sell
Systems for managing and querying semistructured-data sources often store data in proprietary object repositories or in a tagged-text format. We describe a technique that can use relational database management systems to store and manage semistructured data. Our technique relies on a mapping between the semistructured data model and the relational data model, expressed in a query language called STORED. When a semistructured data instance is given, a STORED mapping can be generated automatically using data-mining techniques. We are interested in applying STORED to XML data, which is an instance of semistructured data. We show how a document-type-descriptor (DTD), when present, can be exploited to further improve performance.


Topic 56
Secure One-Round Tripartite Authenticated Key Agreement Protocol from Weil Pairing
With increasing amount of data being stored in XML format, OLAP queries over these data become important. OLAP queries have been well studied in the relational database systems. However, the evaluation of OLAP queries over XML data is not a trivial extension of the relational solutions, especially when a schema is not available. In this paper, we introduce the IX-cube (Iceberg XML cube) over XML data to tackle the problem. We extend OLAP operations to XML data. We also develop efficient approaches to IX-Cube computation and OLAP query evaluation using IX-cubes.

On the Security of Some Password Authentication Protocols
Data warehouses using a multidimensional view of data have become very popular in both business and science in recent years. Data warehouses for scientific purposes such as medicine and bio-chemistry1 pose several great challenges to existing data warehouse technology. Data warehouses usually use pre-aggregated data to ensure fast query response. However, pre-aggregation cannot be used in practice if the dimension structures or the relationships between facts and dimensions are irregular. A technique for overcoming this limitation and some experimental results are presented. Queries over scientific data warehouses often need to reference data that is external to the data warehouse, e.g., data that is too complex to be handled by current data warehouse technology, data that is ”owned” by other organizations, or data that is updated frequently. This paper presents a federation architecture that allows the integration of multidimensional warehouse data with complex external data.

Revocation of privacy-enhanced public-key certificates
Many large organizations have developed data warehouses to support decision making. The data in a warehouse are subject oriented, integrated, time variant, and nonvolatile. A data warehouse contains five types of data: current detail data, older detail data, lightly summarized data, highly summarized data, and metadata. The architecture of a data warehouse includes a backend process (the extraction of data from source systems), the warehouse, and the front-end use (the accessing of data from the warehouse). A data mart is a smaller version of a data warehouse that supports the narrower set of requirements of a single business unit. Data marts should be developed in an integrated manner in order to avoid repeating the "silos of information" problem.An operational data store is a database for transaction processing systems that uses the data warehouse approach to provide clean data. Data warehousing is constantly changing, with the associated opportunities for practice and research, such as the potential for knowledge management using the warehouse.

Weak-Key Classes of 7-Round MISTY 1 and 2 for Related-Key Amplified Boomerang Attacks
Data warehouses contain large amounts of information, often collected from a variety of independent sources. Decision-support functions in a warehouse, such as on-line analytical processing (OLAP), involve hundreds of complex aggregate queries over large volumes of data. It is not feasible to compute these queries by scanning the data sets each time. Warehouse applications therefore build a large number of summary tables, or materialized aggregate views, to help them increase the system performance. As changes, most notably new transactional data, are collected at the data sources, all summary tables at the warehouse that depend upon this data need to be updated. Usually, source changes are loaded into the warehouse at regular intervals, usually once a day, in a batch window, and the warehouse is made unavailable for querying while it is updated. Since the number of summary tables that need to be maintained is often large, a critical issue for data warehousing is how to maintain the summary tables efficiently. In this paper we propose a method of maintaining aggregate views (the summary-delta table method), and use it to solve two problems in maintaining summary tables in a warehouse: (1) how to efficiently maintain a summary table while minimizing the batch window needed for maintenance, and (2) how to maintain a large set of summary tables defined over the same base tables. While several papers have addressed the issues relating to choosing and materializing a set of summary tables, this is the first paper to address maintaining summary tables efficiently.

Enhanced privacy id: a direct anonymous attestation scheme with enhanced revocation capabilities
Null

A Secure Authenticated Key Agreement Protocol Based on Elliptic Curve Cryptography
Data warehouse refreshment is often viewed as a problem of maintaining materialized views over operational sources. In this paper, we show that the data warehouse refreshment process is a complex process comprising several tasks, e.g.,monitoring, extracting, transforming, integrating and cleaning operational data, deriving new data, building histories and loading the data warehouse. We propose a novel approach for defining and executing the refreshment process based on specifications stored in an object-oriented metadata repository. Our approach considers the multidimensional character of OLAP data and can be used in conjunction with various operational sources and target data warehouses.

Password-authenticated 3PEKE with Round Efficiency without Server's Public Key
Data compression is an effective technique to improve the performance of data warehouses. Aggregation and cube are important operations for on-line analytical processing (OLAP). It is a major challenge to develop efficient algorithms for aggregation and cube operations on compressed data warehouses. Many efficient algorithms to compute aggregation and cube for relational OLAP have been developed. Some work has been done on efficiently computing aggregation and cube for multidimensional data warehouses (MDWs) that store datasets in multidimensional arrays rather than in tables. However, to our knowledge, there is few to date in the literature describing aggregation algorithms on compressed data warehouses for multidimensional OLAP.

Secure Deniable Authenticated Key Establishment for Internet Protocols
Null

Three-party encrypted key exchange: attacks and a solution
Three books that set the standard with their groundbreaking methods and techniques for data warehousing. Great value! Save $20, value of the set is $165. This set includes the following books from the Kimball Group: The Data Warehouse Toolkit, Second Edition This authoritative guide presents clear-cut guidelines for designing dimensional models through the use of real-world data warehouse case studies drawn from a variety of business application areas and industries. Ralph Kimball and Margy Ross help you master the full range of powerful techniques for designing dimensional databases that are easy to understand and provide fast query responses. The Data Warehouse Lifecycle Toolkit, Second Edition With this second edition, Ralph Kimball and his Kimball Group colleagues refined the original set of Lifecycle methods and techniques based on their consulting and training experiences. They walk you through detailed steps of designing, developing, and deploying a data warehousing/business intelligence system, from business requirements gathering through delivery, with the steadfast goal of enabling users to make better, more informed business decisions. The Data Warehouse ETL Toolkit Serving as a road map for planning, designing, building, and running the backroom of a data warehouse, this book provides complete coverage of proven, time-saving ETL techniques. You'll discover how a properly designed ETL system extracts data from the source systems, enforces data quality and consistency standards, conforms the data so that separate sources can be used together, and finally delivers the data in a presentation-ready format

Security weakness in a three-party pairing-based protocol for password authenticated key exchange
Systems for managing and querying semistructured-data sources often store data in proprietary object repositories or in a tagged-text format. We describe a technique that can use relational database management systems to store and manage semistructured data. Our technique relies on a mapping between the semistructured data model and the relational data model, expressed in a query language called STORED. When a semistructured data instance is given, a STORED mapping can be generated automatically using data-mining techniques. We are interested in applying STORED to XML data, which is an instance of semistructured data. We show how a document-type-descriptor (DTD), when present, can be exploited to further improve performance.


Topic 63
Alternative operating systems, part 1: The QNX operating system
With increasing amount of data being stored in XML format, OLAP queries over these data become important. OLAP queries have been well studied in the relational database systems. However, the evaluation of OLAP queries over XML data is not a trivial extension of the relational solutions, especially when a schema is not available. In this paper, we introduce the IX-cube (Iceberg XML cube) over XML data to tackle the problem. We extend OLAP operations to XML data. We also develop efficient approaches to IX-Cube computation and OLAP query evaluation using IX-cubes.

An operating systems vade mecum
Data warehouses using a multidimensional view of data have become very popular in both business and science in recent years. Data warehouses for scientific purposes such as medicine and bio-chemistry1 pose several great challenges to existing data warehouse technology. Data warehouses usually use pre-aggregated data to ensure fast query response. However, pre-aggregation cannot be used in practice if the dimension structures or the relationships between facts and dimensions are irregular. A technique for overcoming this limitation and some experimental results are presented. Queries over scientific data warehouses often need to reference data that is external to the data warehouse, e.g., data that is too complex to be handled by current data warehouse technology, data that is ”owned” by other organizations, or data that is updated frequently. This paper presents a federation architecture that allows the integration of multidimensional warehouse data with complex external data.

Experience with MINIX in an operating systems lab
Many large organizations have developed data warehouses to support decision making. The data in a warehouse are subject oriented, integrated, time variant, and nonvolatile. A data warehouse contains five types of data: current detail data, older detail data, lightly summarized data, highly summarized data, and metadata. The architecture of a data warehouse includes a backend process (the extraction of data from source systems), the warehouse, and the front-end use (the accessing of data from the warehouse). A data mart is a smaller version of a data warehouse that supports the narrower set of requirements of a single business unit. Data marts should be developed in an integrated manner in order to avoid repeating the "silos of information" problem.An operational data store is a database for transaction processing systems that uses the data warehouse approach to provide clean data. Data warehousing is constantly changing, with the associated opportunities for practice and research, such as the potential for knowledge management using the warehouse.

Operating Systems for the Micronet Network Computer
Data warehouses contain large amounts of information, often collected from a variety of independent sources. Decision-support functions in a warehouse, such as on-line analytical processing (OLAP), involve hundreds of complex aggregate queries over large volumes of data. It is not feasible to compute these queries by scanning the data sets each time. Warehouse applications therefore build a large number of summary tables, or materialized aggregate views, to help them increase the system performance. As changes, most notably new transactional data, are collected at the data sources, all summary tables at the warehouse that depend upon this data need to be updated. Usually, source changes are loaded into the warehouse at regular intervals, usually once a day, in a batch window, and the warehouse is made unavailable for querying while it is updated. Since the number of summary tables that need to be maintained is often large, a critical issue for data warehousing is how to maintain the summary tables efficiently. In this paper we propose a method of maintaining aggregate views (the summary-delta table method), and use it to solve two problems in maintaining summary tables in a warehouse: (1) how to efficiently maintain a summary table while minimizing the batch window needed for maintenance, and (2) how to maintain a large set of summary tables defined over the same base tables. While several papers have addressed the issues relating to choosing and materializing a set of summary tables, this is the first paper to address maintaining summary tables efficiently.

Heart: An operating system nucleus machine implemented by firmware
Null

RBCS/RCMAS&mdash;converting to the MERT operating system
Data warehouse refreshment is often viewed as a problem of maintaining materialized views over operational sources. In this paper, we show that the data warehouse refreshment process is a complex process comprising several tasks, e.g.,monitoring, extracting, transforming, integrating and cleaning operational data, deriving new data, building histories and loading the data warehouse. We propose a novel approach for defining and executing the refreshment process based on specifications stored in an object-oriented metadata repository. Our approach considers the multidimensional character of OLAP data and can be used in conjunction with various operational sources and target data warehouses.

Operating systems with POPSS
Data compression is an effective technique to improve the performance of data warehouses. Aggregation and cube are important operations for on-line analytical processing (OLAP). It is a major challenge to develop efficient algorithms for aggregation and cube operations on compressed data warehouses. Many efficient algorithms to compute aggregation and cube for relational OLAP have been developed. Some work has been done on efficiently computing aggregation and cube for multidimensional data warehouses (MDWs) that store datasets in multidimensional arrays rather than in tables. However, to our knowledge, there is few to date in the literature describing aggregation algorithms on compressed data warehouses for multidimensional OLAP.

Operating System Security, 1st edition
Null

Operating system: Savaje technologies Savaje Xe
Three books that set the standard with their groundbreaking methods and techniques for data warehousing. Great value! Save $20, value of the set is $165. This set includes the following books from the Kimball Group: The Data Warehouse Toolkit, Second Edition This authoritative guide presents clear-cut guidelines for designing dimensional models through the use of real-world data warehouse case studies drawn from a variety of business application areas and industries. Ralph Kimball and Margy Ross help you master the full range of powerful techniques for designing dimensional databases that are easy to understand and provide fast query responses. The Data Warehouse Lifecycle Toolkit, Second Edition With this second edition, Ralph Kimball and his Kimball Group colleagues refined the original set of Lifecycle methods and techniques based on their consulting and training experiences. They walk you through detailed steps of designing, developing, and deploying a data warehousing/business intelligence system, from business requirements gathering through delivery, with the steadfast goal of enabling users to make better, more informed business decisions. The Data Warehouse ETL Toolkit Serving as a road map for planning, designing, building, and running the backroom of a data warehouse, this book provides complete coverage of proven, time-saving ETL techniques. You'll discover how a properly designed ETL system extracts data from the source systems, enforces data quality and consistency standards, conforms the data so that separate sources can be used together, and finally delivers the data in a presentation-ready format

An operating systems vade mecum; (2nd ed.)
Systems for managing and querying semistructured-data sources often store data in proprietary object repositories or in a tagged-text format. We describe a technique that can use relational database management systems to store and manage semistructured data. Our technique relies on a mapping between the semistructured data model and the relational data model, expressed in a query language called STORED. When a semistructured data instance is given, a STORED mapping can be generated automatically using data-mining techniques. We are interested in applying STORED to XML data, which is an instance of semistructured data. We show how a document-type-descriptor (DTD), when present, can be exploited to further improve performance.


Topic 65
The effect of course sequence on the retention of freshmen engineering students: when should the intro engineering course be offered?
With increasing amount of data being stored in XML format, OLAP queries over these data become important. OLAP queries have been well studied in the relational database systems. However, the evaluation of OLAP queries over XML data is not a trivial extension of the relational solutions, especially when a schema is not available. In this paper, we introduce the IX-cube (Iceberg XML cube) over XML data to tackle the problem. We extend OLAP operations to XML data. We also develop efficient approaches to IX-Cube computation and OLAP query evaluation using IX-cubes.

Curriculum integration in the freshman year at the University of Alabama-foundation coalition program
Data warehouses using a multidimensional view of data have become very popular in both business and science in recent years. Data warehouses for scientific purposes such as medicine and bio-chemistry1 pose several great challenges to existing data warehouse technology. Data warehouses usually use pre-aggregated data to ensure fast query response. However, pre-aggregation cannot be used in practice if the dimension structures or the relationships between facts and dimensions are irregular. A technique for overcoming this limitation and some experimental results are presented. Queries over scientific data warehouses often need to reference data that is external to the data warehouse, e.g., data that is too complex to be handled by current data warehouse technology, data that is ”owned” by other organizations, or data that is updated frequently. This paper presents a federation architecture that allows the integration of multidimensional warehouse data with complex external data.

Augustana women in computer science (AWCS): a program to encourage women in the pursuit of technical education and careers
Many large organizations have developed data warehouses to support decision making. The data in a warehouse are subject oriented, integrated, time variant, and nonvolatile. A data warehouse contains five types of data: current detail data, older detail data, lightly summarized data, highly summarized data, and metadata. The architecture of a data warehouse includes a backend process (the extraction of data from source systems), the warehouse, and the front-end use (the accessing of data from the warehouse). A data mart is a smaller version of a data warehouse that supports the narrower set of requirements of a single business unit. Data marts should be developed in an integrated manner in order to avoid repeating the "silos of information" problem.An operational data store is a database for transaction processing systems that uses the data warehouse approach to provide clean data. Data warehousing is constantly changing, with the associated opportunities for practice and research, such as the potential for knowledge management using the warehouse.

An engineering skills and design course for sophomores with seniors as mentors
Data warehouses contain large amounts of information, often collected from a variety of independent sources. Decision-support functions in a warehouse, such as on-line analytical processing (OLAP), involve hundreds of complex aggregate queries over large volumes of data. It is not feasible to compute these queries by scanning the data sets each time. Warehouse applications therefore build a large number of summary tables, or materialized aggregate views, to help them increase the system performance. As changes, most notably new transactional data, are collected at the data sources, all summary tables at the warehouse that depend upon this data need to be updated. Usually, source changes are loaded into the warehouse at regular intervals, usually once a day, in a batch window, and the warehouse is made unavailable for querying while it is updated. Since the number of summary tables that need to be maintained is often large, a critical issue for data warehousing is how to maintain the summary tables efficiently. In this paper we propose a method of maintaining aggregate views (the summary-delta table method), and use it to solve two problems in maintaining summary tables in a warehouse: (1) how to efficiently maintain a summary table while minimizing the batch window needed for maintenance, and (2) how to maintain a large set of summary tables defined over the same base tables. While several papers have addressed the issues relating to choosing and materializing a set of summary tables, this is the first paper to address maintaining summary tables efficiently.

Building a pipeline of future college engineering students
Null

Developing and implementing a successful research experience for undergraduates program: a roadmap designed by the Engineering Research Center for Particle Science and Technology
Data warehouse refreshment is often viewed as a problem of maintaining materialized views over operational sources. In this paper, we show that the data warehouse refreshment process is a complex process comprising several tasks, e.g.,monitoring, extracting, transforming, integrating and cleaning operational data, deriving new data, building histories and loading the data warehouse. We propose a novel approach for defining and executing the refreshment process based on specifications stored in an object-oriented metadata repository. Our approach considers the multidimensional character of OLAP data and can be used in conjunction with various operational sources and target data warehouses.

Establishing engineering design competencies for freshman/sophomore students
Data compression is an effective technique to improve the performance of data warehouses. Aggregation and cube are important operations for on-line analytical processing (OLAP). It is a major challenge to develop efficient algorithms for aggregation and cube operations on compressed data warehouses. Many efficient algorithms to compute aggregation and cube for relational OLAP have been developed. Some work has been done on efficiently computing aggregation and cube for multidimensional data warehouses (MDWs) that store datasets in multidimensional arrays rather than in tables. However, to our knowledge, there is few to date in the literature describing aggregation algorithms on compressed data warehouses for multidimensional OLAP.

Hands-on science for grades K-5: an industry, school, university collaboration
Null

REU PACI---undergraduate student research experiences
Three books that set the standard with their groundbreaking methods and techniques for data warehousing. Great value! Save $20, value of the set is $165. This set includes the following books from the Kimball Group: The Data Warehouse Toolkit, Second Edition This authoritative guide presents clear-cut guidelines for designing dimensional models through the use of real-world data warehouse case studies drawn from a variety of business application areas and industries. Ralph Kimball and Margy Ross help you master the full range of powerful techniques for designing dimensional databases that are easy to understand and provide fast query responses. The Data Warehouse Lifecycle Toolkit, Second Edition With this second edition, Ralph Kimball and his Kimball Group colleagues refined the original set of Lifecycle methods and techniques based on their consulting and training experiences. They walk you through detailed steps of designing, developing, and deploying a data warehousing/business intelligence system, from business requirements gathering through delivery, with the steadfast goal of enabling users to make better, more informed business decisions. The Data Warehouse ETL Toolkit Serving as a road map for planning, designing, building, and running the backroom of a data warehouse, this book provides complete coverage of proven, time-saving ETL techniques. You'll discover how a properly designed ETL system extracts data from the source systems, enforces data quality and consistency standards, conforms the data so that separate sources can be used together, and finally delivers the data in a presentation-ready format

Camp REACH: an engineering summer camp for middle school girls
Systems for managing and querying semistructured-data sources often store data in proprietary object repositories or in a tagged-text format. We describe a technique that can use relational database management systems to store and manage semistructured data. Our technique relies on a mapping between the semistructured data model and the relational data model, expressed in a query language called STORED. When a semistructured data instance is given, a STORED mapping can be generated automatically using data-mining techniques. We are interested in applying STORED to XML data, which is an instance of semistructured data. We show how a document-type-descriptor (DTD), when present, can be exploited to further improve performance.


Topic 75
Dynamic Integrated Scheduling of Hard Real-Time, Soft Real-Time and Non-Real-Time Processes
With increasing amount of data being stored in XML format, OLAP queries over these data become important. OLAP queries have been well studied in the relational database systems. However, the evaluation of OLAP queries over XML data is not a trivial extension of the relational solutions, especially when a schema is not available. In this paper, we introduce the IX-cube (Iceberg XML cube) over XML data to tackle the problem. We extend OLAP operations to XML data. We also develop efficient approaches to IX-Cube computation and OLAP query evaluation using IX-cubes.

Hierarchical Time-Stamp Protocol: Acquiring Reliable Time-stamp from Local Time Stamping Server
Data warehouses using a multidimensional view of data have become very popular in both business and science in recent years. Data warehouses for scientific purposes such as medicine and bio-chemistry1 pose several great challenges to existing data warehouse technology. Data warehouses usually use pre-aggregated data to ensure fast query response. However, pre-aggregation cannot be used in practice if the dimension structures or the relationships between facts and dimensions are irregular. A technique for overcoming this limitation and some experimental results are presented. Queries over scientific data warehouses often need to reference data that is external to the data warehouse, e.g., data that is too complex to be handled by current data warehouse technology, data that is ”owned” by other organizations, or data that is updated frequently. This paper presents a federation architecture that allows the integration of multidimensional warehouse data with complex external data.

Hardware support for WCET analysis of hard real-time multicore systems
Many large organizations have developed data warehouses to support decision making. The data in a warehouse are subject oriented, integrated, time variant, and nonvolatile. A data warehouse contains five types of data: current detail data, older detail data, lightly summarized data, highly summarized data, and metadata. The architecture of a data warehouse includes a backend process (the extraction of data from source systems), the warehouse, and the front-end use (the accessing of data from the warehouse). A data mart is a smaller version of a data warehouse that supports the narrower set of requirements of a single business unit. Data marts should be developed in an integrated manner in order to avoid repeating the "silos of information" problem.An operational data store is a database for transaction processing systems that uses the data warehouse approach to provide clean data. Data warehousing is constantly changing, with the associated opportunities for practice and research, such as the potential for knowledge management using the warehouse.

Co-Scheduling Variable Execution Time Requirement Real-Time Tasks and Non Real-Time Tasks
Data warehouses contain large amounts of information, often collected from a variety of independent sources. Decision-support functions in a warehouse, such as on-line analytical processing (OLAP), involve hundreds of complex aggregate queries over large volumes of data. It is not feasible to compute these queries by scanning the data sets each time. Warehouse applications therefore build a large number of summary tables, or materialized aggregate views, to help them increase the system performance. As changes, most notably new transactional data, are collected at the data sources, all summary tables at the warehouse that depend upon this data need to be updated. Usually, source changes are loaded into the warehouse at regular intervals, usually once a day, in a batch window, and the warehouse is made unavailable for querying while it is updated. Since the number of summary tables that need to be maintained is often large, a critical issue for data warehousing is how to maintain the summary tables efficiently. In this paper we propose a method of maintaining aggregate views (the summary-delta table method), and use it to solve two problems in maintaining summary tables in a warehouse: (1) how to efficiently maintain a summary table while minimizing the batch window needed for maintenance, and (2) how to maintain a large set of summary tables defined over the same base tables. While several papers have addressed the issues relating to choosing and materializing a set of summary tables, this is the first paper to address maintaining summary tables efficiently.

Flexible Real-Time Linux*: A Flexible Hard Real-Time Environment
Null

Multiform Time in UML for Real-time Embedded Applications
Data warehouse refreshment is often viewed as a problem of maintaining materialized views over operational sources. In this paper, we show that the data warehouse refreshment process is a complex process comprising several tasks, e.g.,monitoring, extracting, transforming, integrating and cleaning operational data, deriving new data, building histories and loading the data warehouse. We propose a novel approach for defining and executing the refreshment process based on specifications stored in an object-oriented metadata repository. Our approach considers the multidimensional character of OLAP data and can be used in conjunction with various operational sources and target data warehouses.

Predictable response times and portable hard real-time systems with TRANS-RTXc on the transputer
Data compression is an effective technique to improve the performance of data warehouses. Aggregation and cube are important operations for on-line analytical processing (OLAP). It is a major challenge to develop efficient algorithms for aggregation and cube operations on compressed data warehouses. Many efficient algorithms to compute aggregation and cube for relational OLAP have been developed. Some work has been done on efficiently computing aggregation and cube for multidimensional data warehouses (MDWs) that store datasets in multidimensional arrays rather than in tables. However, to our knowledge, there is few to date in the literature describing aggregation algorithms on compressed data warehouses for multidimensional OLAP.

Computing Accumulated Delays in Real-time Systems
Null

Deferring Real-Time Traffic for Improved Non-Real-Time Communication in FDDI Networks
Three books that set the standard with their groundbreaking methods and techniques for data warehousing. Great value! Save $20, value of the set is $165. This set includes the following books from the Kimball Group: The Data Warehouse Toolkit, Second Edition This authoritative guide presents clear-cut guidelines for designing dimensional models through the use of real-world data warehouse case studies drawn from a variety of business application areas and industries. Ralph Kimball and Margy Ross help you master the full range of powerful techniques for designing dimensional databases that are easy to understand and provide fast query responses. The Data Warehouse Lifecycle Toolkit, Second Edition With this second edition, Ralph Kimball and his Kimball Group colleagues refined the original set of Lifecycle methods and techniques based on their consulting and training experiences. They walk you through detailed steps of designing, developing, and deploying a data warehousing/business intelligence system, from business requirements gathering through delivery, with the steadfast goal of enabling users to make better, more informed business decisions. The Data Warehouse ETL Toolkit Serving as a road map for planning, designing, building, and running the backroom of a data warehouse, this book provides complete coverage of proven, time-saving ETL techniques. You'll discover how a properly designed ETL system extracts data from the source systems, enforces data quality and consistency standards, conforms the data so that separate sources can be used together, and finally delivers the data in a presentation-ready format

RTZen: highly predictable, real-time java middleware for distributed and embedded systems
Systems for managing and querying semistructured-data sources often store data in proprietary object repositories or in a tagged-text format. We describe a technique that can use relational database management systems to store and manage semistructured data. Our technique relies on a mapping between the semistructured data model and the relational data model, expressed in a query language called STORED. When a semistructured data instance is given, a STORED mapping can be generated automatically using data-mining techniques. We are interested in applying STORED to XML data, which is an instance of semistructured data. We show how a document-type-descriptor (DTD), when present, can be exploited to further improve performance.


Topic 77
Oracle PL/SQL Tips and Techniques, 1st edition
With increasing amount of data being stored in XML format, OLAP queries over these data become important. OLAP queries have been well studied in the relational database systems. However, the evaluation of OLAP queries over XML data is not a trivial extension of the relational solutions, especially when a schema is not available. In this paper, we introduce the IX-cube (Iceberg XML cube) over XML data to tackle the problem. We extend OLAP operations to XML data. We also develop efficient approaches to IX-Cube computation and OLAP query evaluation using IX-cubes.

Oracle Database 10g OCP Certification All-In-One Exam Guide, 1 edition
Data warehouses using a multidimensional view of data have become very popular in both business and science in recent years. Data warehouses for scientific purposes such as medicine and bio-chemistry1 pose several great challenges to existing data warehouse technology. Data warehouses usually use pre-aggregated data to ensure fast query response. However, pre-aggregation cannot be used in practice if the dimension structures or the relationships between facts and dimensions are irregular. A technique for overcoming this limitation and some experimental results are presented. Queries over scientific data warehouses often need to reference data that is external to the data warehouse, e.g., data that is too complex to be handled by current data warehouse technology, data that is ”owned” by other organizations, or data that is updated frequently. This paper presents a federation architecture that allows the integration of multidimensional warehouse data with complex external data.

REL-NER: a tool for converting a relational database schema into a nested entity-relationship schema
Many large organizations have developed data warehouses to support decision making. The data in a warehouse are subject oriented, integrated, time variant, and nonvolatile. A data warehouse contains five types of data: current detail data, older detail data, lightly summarized data, highly summarized data, and metadata. The architecture of a data warehouse includes a backend process (the extraction of data from source systems), the warehouse, and the front-end use (the accessing of data from the warehouse). A data mart is a smaller version of a data warehouse that supports the narrower set of requirements of a single business unit. Data marts should be developed in an integrated manner in order to avoid repeating the "silos of information" problem.An operational data store is a database for transaction processing systems that uses the data warehouse approach to provide clean data. Data warehousing is constantly changing, with the associated opportunities for practice and research, such as the potential for knowledge management using the warehouse.

Creating a Self-Tuning Oracle Database: Automating Oracle9i Dynamic Sga Performance
Data warehouses contain large amounts of information, often collected from a variety of independent sources. Decision-support functions in a warehouse, such as on-line analytical processing (OLAP), involve hundreds of complex aggregate queries over large volumes of data. It is not feasible to compute these queries by scanning the data sets each time. Warehouse applications therefore build a large number of summary tables, or materialized aggregate views, to help them increase the system performance. As changes, most notably new transactional data, are collected at the data sources, all summary tables at the warehouse that depend upon this data need to be updated. Usually, source changes are loaded into the warehouse at regular intervals, usually once a day, in a batch window, and the warehouse is made unavailable for querying while it is updated. Since the number of summary tables that need to be maintained is often large, a critical issue for data warehousing is how to maintain the summary tables efficiently. In this paper we propose a method of maintaining aggregate views (the summary-delta table method), and use it to solve two problems in maintaining summary tables in a warehouse: (1) how to efficiently maintain a summary table while minimizing the batch window needed for maintenance, and (2) how to maintain a large set of summary tables defined over the same base tables. While several papers have addressed the issues relating to choosing and materializing a set of summary tables, this is the first paper to address maintaining summary tables efficiently.

Hands-On Oracle Database 10g Express Edition for Windows, 1 edition
Null

CQLF---a query language for CODASYL-type databases
Data warehouse refreshment is often viewed as a problem of maintaining materialized views over operational sources. In this paper, we show that the data warehouse refreshment process is a complex process comprising several tasks, e.g.,monitoring, extracting, transforming, integrating and cleaning operational data, deriving new data, building histories and loading the data warehouse. We propose a novel approach for defining and executing the refreshment process based on specifications stored in an object-oriented metadata repository. Our approach considers the multidimensional character of OLAP data and can be used in conjunction with various operational sources and target data warehouses.

Oracle9i PL/SQL: A Developer's Guide
Data compression is an effective technique to improve the performance of data warehouses. Aggregation and cube are important operations for on-line analytical processing (OLAP). It is a major challenge to develop efficient algorithms for aggregation and cube operations on compressed data warehouses. Many efficient algorithms to compute aggregation and cube for relational OLAP have been developed. Some work has been done on efficiently computing aggregation and cube for multidimensional data warehouses (MDWs) that store datasets in multidimensional arrays rather than in tables. However, to our knowledge, there is few to date in the literature describing aggregation algorithms on compressed data warehouses for multidimensional OLAP.

Viaggio Lungo il Nilo: OLE DB and Component Databases (abstract only)
Null

Oracle PL/SQL for DBAs
Three books that set the standard with their groundbreaking methods and techniques for data warehousing. Great value! Save $20, value of the set is $165. This set includes the following books from the Kimball Group: The Data Warehouse Toolkit, Second Edition This authoritative guide presents clear-cut guidelines for designing dimensional models through the use of real-world data warehouse case studies drawn from a variety of business application areas and industries. Ralph Kimball and Margy Ross help you master the full range of powerful techniques for designing dimensional databases that are easy to understand and provide fast query responses. The Data Warehouse Lifecycle Toolkit, Second Edition With this second edition, Ralph Kimball and his Kimball Group colleagues refined the original set of Lifecycle methods and techniques based on their consulting and training experiences. They walk you through detailed steps of designing, developing, and deploying a data warehousing/business intelligence system, from business requirements gathering through delivery, with the steadfast goal of enabling users to make better, more informed business decisions. The Data Warehouse ETL Toolkit Serving as a road map for planning, designing, building, and running the backroom of a data warehouse, this book provides complete coverage of proven, time-saving ETL techniques. You'll discover how a properly designed ETL system extracts data from the source systems, enforces data quality and consistency standards, conforms the data so that separate sources can be used together, and finally delivers the data in a presentation-ready format

Hands-On Oracle Database 10g Express Edition for Linux, 1 edition
Systems for managing and querying semistructured-data sources often store data in proprietary object repositories or in a tagged-text format. We describe a technique that can use relational database management systems to store and manage semistructured data. Our technique relies on a mapping between the semistructured data model and the relational data model, expressed in a query language called STORED. When a semistructured data instance is given, a STORED mapping can be generated automatically using data-mining techniques. We are interested in applying STORED to XML data, which is an instance of semistructured data. We show how a document-type-descriptor (DTD), when present, can be exploited to further improve performance.


Topic 78
A unified Approach for Software Architecture Evolution at different abstraction levels
With increasing amount of data being stored in XML format, OLAP queries over these data become important. OLAP queries have been well studied in the relational database systems. However, the evaluation of OLAP queries over XML data is not a trivial extension of the relational solutions, especially when a schema is not available. In this paper, we introduce the IX-cube (Iceberg XML cube) over XML data to tackle the problem. We extend OLAP operations to XML data. We also develop efficient approaches to IX-Cube computation and OLAP query evaluation using IX-cubes.

S*(QM-1): An instantiation of the high level microprogramming language scheme S* for the nanodata QM-11
Data warehouses using a multidimensional view of data have become very popular in both business and science in recent years. Data warehouses for scientific purposes such as medicine and bio-chemistry1 pose several great challenges to existing data warehouse technology. Data warehouses usually use pre-aggregated data to ensure fast query response. However, pre-aggregation cannot be used in practice if the dimension structures or the relationships between facts and dimensions are irregular. A technique for overcoming this limitation and some experimental results are presented. Queries over scientific data warehouses often need to reference data that is external to the data warehouse, e.g., data that is too complex to be handled by current data warehouse technology, data that is ”owned” by other organizations, or data that is updated frequently. This paper presents a federation architecture that allows the integration of multidimensional warehouse data with complex external data.

Experience with a high level micromachine simulator
Many large organizations have developed data warehouses to support decision making. The data in a warehouse are subject oriented, integrated, time variant, and nonvolatile. A data warehouse contains five types of data: current detail data, older detail data, lightly summarized data, highly summarized data, and metadata. The architecture of a data warehouse includes a backend process (the extraction of data from source systems), the warehouse, and the front-end use (the accessing of data from the warehouse). A data mart is a smaller version of a data warehouse that supports the narrower set of requirements of a single business unit. Data marts should be developed in an integrated manner in order to avoid repeating the "silos of information" problem.An operational data store is a database for transaction processing systems that uses the data warehouse approach to provide clean data. Data warehousing is constantly changing, with the associated opportunities for practice and research, such as the potential for knowledge management using the warehouse.

Architecture exploration of parameterizable EPIC SOS architectures (poster paper)
Data warehouses contain large amounts of information, often collected from a variety of independent sources. Decision-support functions in a warehouse, such as on-line analytical processing (OLAP), involve hundreds of complex aggregate queries over large volumes of data. It is not feasible to compute these queries by scanning the data sets each time. Warehouse applications therefore build a large number of summary tables, or materialized aggregate views, to help them increase the system performance. As changes, most notably new transactional data, are collected at the data sources, all summary tables at the warehouse that depend upon this data need to be updated. Usually, source changes are loaded into the warehouse at regular intervals, usually once a day, in a batch window, and the warehouse is made unavailable for querying while it is updated. Since the number of summary tables that need to be maintained is often large, a critical issue for data warehousing is how to maintain the summary tables efficiently. In this paper we propose a method of maintaining aggregate views (the summary-delta table method), and use it to solve two problems in maintaining summary tables in a warehouse: (1) how to efficiently maintain a summary table while minimizing the batch window needed for maintenance, and (2) how to maintain a large set of summary tables defined over the same base tables. While several papers have addressed the issues relating to choosing and materializing a set of summary tables, this is the first paper to address maintaining summary tables efficiently.

Enhanced connectors to support hierarchical dependencies in software architecture
Null

MetaRTL: raising the abstraction level of RTL design
Data warehouse refreshment is often viewed as a problem of maintaining materialized views over operational sources. In this paper, we show that the data warehouse refreshment process is a complex process comprising several tasks, e.g.,monitoring, extracting, transforming, integrating and cleaning operational data, deriving new data, building histories and loading the data warehouse. We propose a novel approach for defining and executing the refreshment process based on specifications stored in an object-oriented metadata repository. Our approach considers the multidimensional character of OLAP data and can be used in conjunction with various operational sources and target data warehouses.

Concepts of high-level-language computer architecture
Data compression is an effective technique to improve the performance of data warehouses. Aggregation and cube are important operations for on-line analytical processing (OLAP). It is a major challenge to develop efficient algorithms for aggregation and cube operations on compressed data warehouses. Many efficient algorithms to compute aggregation and cube for relational OLAP have been developed. Some work has been done on efficiently computing aggregation and cube for multidimensional data warehouses (MDWs) that store datasets in multidimensional arrays rather than in tables. However, to our knowledge, there is few to date in the literature describing aggregation algorithms on compressed data warehouses for multidimensional OLAP.

A high-level-language programmable controller, part II&mdash;microcompilation of the high-level language micropascal
Null

MADL: Meta Architecture Description Language
Three books that set the standard with their groundbreaking methods and techniques for data warehousing. Great value! Save $20, value of the set is $165. This set includes the following books from the Kimball Group: The Data Warehouse Toolkit, Second Edition This authoritative guide presents clear-cut guidelines for designing dimensional models through the use of real-world data warehouse case studies drawn from a variety of business application areas and industries. Ralph Kimball and Margy Ross help you master the full range of powerful techniques for designing dimensional databases that are easy to understand and provide fast query responses. The Data Warehouse Lifecycle Toolkit, Second Edition With this second edition, Ralph Kimball and his Kimball Group colleagues refined the original set of Lifecycle methods and techniques based on their consulting and training experiences. They walk you through detailed steps of designing, developing, and deploying a data warehousing/business intelligence system, from business requirements gathering through delivery, with the steadfast goal of enabling users to make better, more informed business decisions. The Data Warehouse ETL Toolkit Serving as a road map for planning, designing, building, and running the backroom of a data warehouse, this book provides complete coverage of proven, time-saving ETL techniques. You'll discover how a properly designed ETL system extracts data from the source systems, enforces data quality and consistency standards, conforms the data so that separate sources can be used together, and finally delivers the data in a presentation-ready format

A hardware implementation of capability-based addressing
Systems for managing and querying semistructured-data sources often store data in proprietary object repositories or in a tagged-text format. We describe a technique that can use relational database management systems to store and manage semistructured data. Our technique relies on a mapping between the semistructured data model and the relational data model, expressed in a query language called STORED. When a semistructured data instance is given, a STORED mapping can be generated automatically using data-mining techniques. We are interested in applying STORED to XML data, which is an instance of semistructured data. We show how a document-type-descriptor (DTD), when present, can be exploited to further improve performance.


Topic 79
Consideration of selected personality - job satisfaction constructs relevant to project management in data processing organizations
With increasing amount of data being stored in XML format, OLAP queries over these data become important. OLAP queries have been well studied in the relational database systems. However, the evaluation of OLAP queries over XML data is not a trivial extension of the relational solutions, especially when a schema is not available. In this paper, we introduce the IX-cube (Iceberg XML cube) over XML data to tackle the problem. We extend OLAP operations to XML data. We also develop efficient approaches to IX-Cube computation and OLAP query evaluation using IX-cubes.

Teamwork Quality and the Success of Innovative Projects: A Theoretical Concept and Empirical Evidence
Data warehouses using a multidimensional view of data have become very popular in both business and science in recent years. Data warehouses for scientific purposes such as medicine and bio-chemistry1 pose several great challenges to existing data warehouse technology. Data warehouses usually use pre-aggregated data to ensure fast query response. However, pre-aggregation cannot be used in practice if the dimension structures or the relationships between facts and dimensions are irregular. A technique for overcoming this limitation and some experimental results are presented. Queries over scientific data warehouses often need to reference data that is external to the data warehouse, e.g., data that is too complex to be handled by current data warehouse technology, data that is ”owned” by other organizations, or data that is updated frequently. This paper presents a federation architecture that allows the integration of multidimensional warehouse data with complex external data.

Distributed Scrum: Agile Project Management with Outsourced Development Teams
Many large organizations have developed data warehouses to support decision making. The data in a warehouse are subject oriented, integrated, time variant, and nonvolatile. A data warehouse contains five types of data: current detail data, older detail data, lightly summarized data, highly summarized data, and metadata. The architecture of a data warehouse includes a backend process (the extraction of data from source systems), the warehouse, and the front-end use (the accessing of data from the warehouse). A data mart is a smaller version of a data warehouse that supports the narrower set of requirements of a single business unit. Data marts should be developed in an integrated manner in order to avoid repeating the "silos of information" problem.An operational data store is a database for transaction processing systems that uses the data warehouse approach to provide clean data. Data warehousing is constantly changing, with the associated opportunities for practice and research, such as the potential for knowledge management using the warehouse.

The 123s of ABC in SAP: Using SAP R/3 to Support Activity-Based Costing
Data warehouses contain large amounts of information, often collected from a variety of independent sources. Decision-support functions in a warehouse, such as on-line analytical processing (OLAP), involve hundreds of complex aggregate queries over large volumes of data. It is not feasible to compute these queries by scanning the data sets each time. Warehouse applications therefore build a large number of summary tables, or materialized aggregate views, to help them increase the system performance. As changes, most notably new transactional data, are collected at the data sources, all summary tables at the warehouse that depend upon this data need to be updated. Usually, source changes are loaded into the warehouse at regular intervals, usually once a day, in a batch window, and the warehouse is made unavailable for querying while it is updated. Since the number of summary tables that need to be maintained is often large, a critical issue for data warehousing is how to maintain the summary tables efficiently. In this paper we propose a method of maintaining aggregate views (the summary-delta table method), and use it to solve two problems in maintaining summary tables in a warehouse: (1) how to efficiently maintain a summary table while minimizing the batch window needed for maintenance, and (2) how to maintain a large set of summary tables defined over the same base tables. While several papers have addressed the issues relating to choosing and materializing a set of summary tables, this is the first paper to address maintaining summary tables efficiently.

Matering HR Management with SAP ERP HCM, 1st edition
Null

Supply-Chain Synchronization: Lessons from Hyundai Motor Company
Data warehouse refreshment is often viewed as a problem of maintaining materialized views over operational sources. In this paper, we show that the data warehouse refreshment process is a complex process comprising several tasks, e.g.,monitoring, extracting, transforming, integrating and cleaning operational data, deriving new data, building histories and loading the data warehouse. We propose a novel approach for defining and executing the refreshment process based on specifications stored in an object-oriented metadata repository. Our approach considers the multidimensional character of OLAP data and can be used in conjunction with various operational sources and target data warehouses.

How to Cheat at IT Project Management
Data compression is an effective technique to improve the performance of data warehouses. Aggregation and cube are important operations for on-line analytical processing (OLAP). It is a major challenge to develop efficient algorithms for aggregation and cube operations on compressed data warehouses. Many efficient algorithms to compute aggregation and cube for relational OLAP have been developed. Some work has been done on efficiently computing aggregation and cube for multidimensional data warehouses (MDWs) that store datasets in multidimensional arrays rather than in tables. However, to our knowledge, there is few to date in the literature describing aggregation algorithms on compressed data warehouses for multidimensional OLAP.

ITIL V3 MALC - Managing Across the Lifecycle of IT Services Best Practices Study and Implementation Guide
Null

Supply Chain Management Based on SAP Systems: Architecture and Planning Processes, 1st edition
Three books that set the standard with their groundbreaking methods and techniques for data warehousing. Great value! Save $20, value of the set is $165. This set includes the following books from the Kimball Group: The Data Warehouse Toolkit, Second Edition This authoritative guide presents clear-cut guidelines for designing dimensional models through the use of real-world data warehouse case studies drawn from a variety of business application areas and industries. Ralph Kimball and Margy Ross help you master the full range of powerful techniques for designing dimensional databases that are easy to understand and provide fast query responses. The Data Warehouse Lifecycle Toolkit, Second Edition With this second edition, Ralph Kimball and his Kimball Group colleagues refined the original set of Lifecycle methods and techniques based on their consulting and training experiences. They walk you through detailed steps of designing, developing, and deploying a data warehousing/business intelligence system, from business requirements gathering through delivery, with the steadfast goal of enabling users to make better, more informed business decisions. The Data Warehouse ETL Toolkit Serving as a road map for planning, designing, building, and running the backroom of a data warehouse, this book provides complete coverage of proven, time-saving ETL techniques. You'll discover how a properly designed ETL system extracts data from the source systems, enforces data quality and consistency standards, conforms the data so that separate sources can be used together, and finally delivers the data in a presentation-ready format

How to Manage a Successful Software Project: With Microsoft Project 2000, 2nd edition
Systems for managing and querying semistructured-data sources often store data in proprietary object repositories or in a tagged-text format. We describe a technique that can use relational database management systems to store and manage semistructured data. Our technique relies on a mapping between the semistructured data model and the relational data model, expressed in a query language called STORED. When a semistructured data instance is given, a STORED mapping can be generated automatically using data-mining techniques. We are interested in applying STORED to XML data, which is an instance of semistructured data. We show how a document-type-descriptor (DTD), when present, can be exploited to further improve performance.


Topic 84
Video query processing in the VDBMS testbed for video database research
With increasing amount of data being stored in XML format, OLAP queries over these data become important. OLAP queries have been well studied in the relational database systems. However, the evaluation of OLAP queries over XML data is not a trivial extension of the relational solutions, especially when a schema is not available. In this paper, we introduce the IX-cube (Iceberg XML cube) over XML data to tackle the problem. We extend OLAP operations to XML data. We also develop efficient approaches to IX-Cube computation and OLAP query evaluation using IX-cubes.

InterJoin: Exploiting Indexes and Materialized Views in XPath Evaluation
Data warehouses using a multidimensional view of data have become very popular in both business and science in recent years. Data warehouses for scientific purposes such as medicine and bio-chemistry1 pose several great challenges to existing data warehouse technology. Data warehouses usually use pre-aggregated data to ensure fast query response. However, pre-aggregation cannot be used in practice if the dimension structures or the relationships between facts and dimensions are irregular. A technique for overcoming this limitation and some experimental results are presented. Queries over scientific data warehouses often need to reference data that is external to the data warehouse, e.g., data that is too complex to be handled by current data warehouse technology, data that is ”owned” by other organizations, or data that is updated frequently. This paper presents a federation architecture that allows the integration of multidimensional warehouse data with complex external data.

A framework for using materialized XPath views in XML query processing
Many large organizations have developed data warehouses to support decision making. The data in a warehouse are subject oriented, integrated, time variant, and nonvolatile. A data warehouse contains five types of data: current detail data, older detail data, lightly summarized data, highly summarized data, and metadata. The architecture of a data warehouse includes a backend process (the extraction of data from source systems), the warehouse, and the front-end use (the accessing of data from the warehouse). A data mart is a smaller version of a data warehouse that supports the narrower set of requirements of a single business unit. Data marts should be developed in an integrated manner in order to avoid repeating the "silos of information" problem.An operational data store is a database for transaction processing systems that uses the data warehouse approach to provide clean data. Data warehousing is constantly changing, with the associated opportunities for practice and research, such as the potential for knowledge management using the warehouse.

How to Determine Output Schemas of XQuery Queries
Data warehouses contain large amounts of information, often collected from a variety of independent sources. Decision-support functions in a warehouse, such as on-line analytical processing (OLAP), involve hundreds of complex aggregate queries over large volumes of data. It is not feasible to compute these queries by scanning the data sets each time. Warehouse applications therefore build a large number of summary tables, or materialized aggregate views, to help them increase the system performance. As changes, most notably new transactional data, are collected at the data sources, all summary tables at the warehouse that depend upon this data need to be updated. Usually, source changes are loaded into the warehouse at regular intervals, usually once a day, in a batch window, and the warehouse is made unavailable for querying while it is updated. Since the number of summary tables that need to be maintained is often large, a critical issue for data warehousing is how to maintain the summary tables efficiently. In this paper we propose a method of maintaining aggregate views (the summary-delta table method), and use it to solve two problems in maintaining summary tables in a warehouse: (1) how to efficiently maintain a summary table while minimizing the batch window needed for maintenance, and (2) how to maintain a large set of summary tables defined over the same base tables. While several papers have addressed the issues relating to choosing and materializing a set of summary tables, this is the first paper to address maintaining summary tables efficiently.

Efficient xpath query processing on stored and streaming xml data
Null

Efficient xpath query processing in native xml databases
Data warehouse refreshment is often viewed as a problem of maintaining materialized views over operational sources. In this paper, we show that the data warehouse refreshment process is a complex process comprising several tasks, e.g.,monitoring, extracting, transforming, integrating and cleaning operational data, deriving new data, building histories and loading the data warehouse. We propose a novel approach for defining and executing the refreshment process based on specifications stored in an object-oriented metadata repository. Our approach considers the multidimensional character of OLAP data and can be used in conjunction with various operational sources and target data warehouses.

Efficient processing of XML twig queries with OR-predicates
Data compression is an effective technique to improve the performance of data warehouses. Aggregation and cube are important operations for on-line analytical processing (OLAP). It is a major challenge to develop efficient algorithms for aggregation and cube operations on compressed data warehouses. Many efficient algorithms to compute aggregation and cube for relational OLAP have been developed. Some work has been done on efficiently computing aggregation and cube for multidimensional data warehouses (MDWs) that store datasets in multidimensional arrays rather than in tables. However, to our knowledge, there is few to date in the literature describing aggregation algorithms on compressed data warehouses for multidimensional OLAP.

Query processing of streamed XML data
Null

Containment of nested XML queries
Three books that set the standard with their groundbreaking methods and techniques for data warehousing. Great value! Save $20, value of the set is $165. This set includes the following books from the Kimball Group: The Data Warehouse Toolkit, Second Edition This authoritative guide presents clear-cut guidelines for designing dimensional models through the use of real-world data warehouse case studies drawn from a variety of business application areas and industries. Ralph Kimball and Margy Ross help you master the full range of powerful techniques for designing dimensional databases that are easy to understand and provide fast query responses. The Data Warehouse Lifecycle Toolkit, Second Edition With this second edition, Ralph Kimball and his Kimball Group colleagues refined the original set of Lifecycle methods and techniques based on their consulting and training experiences. They walk you through detailed steps of designing, developing, and deploying a data warehousing/business intelligence system, from business requirements gathering through delivery, with the steadfast goal of enabling users to make better, more informed business decisions. The Data Warehouse ETL Toolkit Serving as a road map for planning, designing, building, and running the backroom of a data warehouse, this book provides complete coverage of proven, time-saving ETL techniques. You'll discover how a properly designed ETL system extracts data from the source systems, enforces data quality and consistency standards, conforms the data so that separate sources can be used together, and finally delivers the data in a presentation-ready format

XML-based information mediation with MIX
Systems for managing and querying semistructured-data sources often store data in proprietary object repositories or in a tagged-text format. We describe a technique that can use relational database management systems to store and manage semistructured data. Our technique relies on a mapping between the semistructured data model and the relational data model, expressed in a query language called STORED. When a semistructured data instance is given, a STORED mapping can be generated automatically using data-mining techniques. We are interested in applying STORED to XML data, which is an instance of semistructured data. We show how a document-type-descriptor (DTD), when present, can be exploited to further improve performance.


Topic 86
Quantum-Inspired Genetic Algorithm Based on Simulated Annealing for Combinatorial Optimization Problem
With increasing amount of data being stored in XML format, OLAP queries over these data become important. OLAP queries have been well studied in the relational database systems. However, the evaluation of OLAP queries over XML data is not a trivial extension of the relational solutions, especially when a schema is not available. In this paper, we introduce the IX-cube (Iceberg XML cube) over XML data to tackle the problem. We extend OLAP operations to XML data. We also develop efficient approaches to IX-Cube computation and OLAP query evaluation using IX-cubes.

A Modified Niche Genetic Algorithm Based on Evolution Gradient and Its Simulation Analysis
Data warehouses using a multidimensional view of data have become very popular in both business and science in recent years. Data warehouses for scientific purposes such as medicine and bio-chemistry1 pose several great challenges to existing data warehouse technology. Data warehouses usually use pre-aggregated data to ensure fast query response. However, pre-aggregation cannot be used in practice if the dimension structures or the relationships between facts and dimensions are irregular. A technique for overcoming this limitation and some experimental results are presented. Queries over scientific data warehouses often need to reference data that is external to the data warehouse, e.g., data that is too complex to be handled by current data warehouse technology, data that is ”owned” by other organizations, or data that is updated frequently. This paper presents a federation architecture that allows the integration of multidimensional warehouse data with complex external data.

A Modified Genetic Algorithm with Multiple Subpopulations and Dynamic Parameters Applied in CVaR Model
Many large organizations have developed data warehouses to support decision making. The data in a warehouse are subject oriented, integrated, time variant, and nonvolatile. A data warehouse contains five types of data: current detail data, older detail data, lightly summarized data, highly summarized data, and metadata. The architecture of a data warehouse includes a backend process (the extraction of data from source systems), the warehouse, and the front-end use (the accessing of data from the warehouse). A data mart is a smaller version of a data warehouse that supports the narrower set of requirements of a single business unit. Data marts should be developed in an integrated manner in order to avoid repeating the "silos of information" problem.An operational data store is a database for transaction processing systems that uses the data warehouse approach to provide clean data. Data warehousing is constantly changing, with the associated opportunities for practice and research, such as the potential for knowledge management using the warehouse.

A gradient method on the initial partition of Fiduccia-Mattheyses algorithm
Data warehouses contain large amounts of information, often collected from a variety of independent sources. Decision-support functions in a warehouse, such as on-line analytical processing (OLAP), involve hundreds of complex aggregate queries over large volumes of data. It is not feasible to compute these queries by scanning the data sets each time. Warehouse applications therefore build a large number of summary tables, or materialized aggregate views, to help them increase the system performance. As changes, most notably new transactional data, are collected at the data sources, all summary tables at the warehouse that depend upon this data need to be updated. Usually, source changes are loaded into the warehouse at regular intervals, usually once a day, in a batch window, and the warehouse is made unavailable for querying while it is updated. Since the number of summary tables that need to be maintained is often large, a critical issue for data warehousing is how to maintain the summary tables efficiently. In this paper we propose a method of maintaining aggregate views (the summary-delta table method), and use it to solve two problems in maintaining summary tables in a warehouse: (1) how to efficiently maintain a summary table while minimizing the batch window needed for maintenance, and (2) how to maintain a large set of summary tables defined over the same base tables. While several papers have addressed the issues relating to choosing and materializing a set of summary tables, this is the first paper to address maintaining summary tables efficiently.

An Application of New Quantum-Inspired Immune Evolutionary Algorithm
Null

hLCGA: A Hybrid Competitive Coevolutionary Genetic Algorithm
Data warehouse refreshment is often viewed as a problem of maintaining materialized views over operational sources. In this paper, we show that the data warehouse refreshment process is a complex process comprising several tasks, e.g.,monitoring, extracting, transforming, integrating and cleaning operational data, deriving new data, building histories and loading the data warehouse. We propose a novel approach for defining and executing the refreshment process based on specifications stored in an object-oriented metadata repository. Our approach considers the multidimensional character of OLAP data and can be used in conjunction with various operational sources and target data warehouses.

A New Mutation Operator for the Elitism-Based Compact Genetic Algorithm
Data compression is an effective technique to improve the performance of data warehouses. Aggregation and cube are important operations for on-line analytical processing (OLAP). It is a major challenge to develop efficient algorithms for aggregation and cube operations on compressed data warehouses. Many efficient algorithms to compute aggregation and cube for relational OLAP have been developed. Some work has been done on efficiently computing aggregation and cube for multidimensional data warehouses (MDWs) that store datasets in multidimensional arrays rather than in tables. However, to our knowledge, there is few to date in the literature describing aggregation algorithms on compressed data warehouses for multidimensional OLAP.

Research on Diversity Measure of Niche Genetic Algorithm
Null

Improved genetic algorithm inspired by biological evolution
Three books that set the standard with their groundbreaking methods and techniques for data warehousing. Great value! Save $20, value of the set is $165. This set includes the following books from the Kimball Group: The Data Warehouse Toolkit, Second Edition This authoritative guide presents clear-cut guidelines for designing dimensional models through the use of real-world data warehouse case studies drawn from a variety of business application areas and industries. Ralph Kimball and Margy Ross help you master the full range of powerful techniques for designing dimensional databases that are easy to understand and provide fast query responses. The Data Warehouse Lifecycle Toolkit, Second Edition With this second edition, Ralph Kimball and his Kimball Group colleagues refined the original set of Lifecycle methods and techniques based on their consulting and training experiences. They walk you through detailed steps of designing, developing, and deploying a data warehousing/business intelligence system, from business requirements gathering through delivery, with the steadfast goal of enabling users to make better, more informed business decisions. The Data Warehouse ETL Toolkit Serving as a road map for planning, designing, building, and running the backroom of a data warehouse, this book provides complete coverage of proven, time-saving ETL techniques. You'll discover how a properly designed ETL system extracts data from the source systems, enforces data quality and consistency standards, conforms the data so that separate sources can be used together, and finally delivers the data in a presentation-ready format

Comparing evolutionary algorithms to the (1+1) -EA
Systems for managing and querying semistructured-data sources often store data in proprietary object repositories or in a tagged-text format. We describe a technique that can use relational database management systems to store and manage semistructured data. Our technique relies on a mapping between the semistructured data model and the relational data model, expressed in a query language called STORED. When a semistructured data instance is given, a STORED mapping can be generated automatically using data-mining techniques. We are interested in applying STORED to XML data, which is an instance of semistructured data. We show how a document-type-descriptor (DTD), when present, can be exploited to further improve performance.


Topic 87
The MAGIC Touch: Combining MAGIC-Pointing with a Touch-Sensitive Mouse
With increasing amount of data being stored in XML format, OLAP queries over these data become important. OLAP queries have been well studied in the relational database systems. However, the evaluation of OLAP queries over XML data is not a trivial extension of the relational solutions, especially when a schema is not available. In this paper, we introduce the IX-cube (Iceberg XML cube) over XML data to tackle the problem. We extend OLAP operations to XML data. We also develop efficient approaches to IX-Cube computation and OLAP query evaluation using IX-cubes.

Determining the benefits of direct-touch, bimanual, and multifinger input on a multitouch workstation
Data warehouses using a multidimensional view of data have become very popular in both business and science in recent years. Data warehouses for scientific purposes such as medicine and bio-chemistry1 pose several great challenges to existing data warehouse technology. Data warehouses usually use pre-aggregated data to ensure fast query response. However, pre-aggregation cannot be used in practice if the dimension structures or the relationships between facts and dimensions are irregular. A technique for overcoming this limitation and some experimental results are presented. Queries over scientific data warehouses often need to reference data that is external to the data warehouse, e.g., data that is too complex to be handled by current data warehouse technology, data that is ”owned” by other organizations, or data that is updated frequently. This paper presents a federation architecture that allows the integration of multidimensional warehouse data with complex external data.

UbiGesture: Customizing and Profiling Hand Gestures in Ubiquitous Environment
Many large organizations have developed data warehouses to support decision making. The data in a warehouse are subject oriented, integrated, time variant, and nonvolatile. A data warehouse contains five types of data: current detail data, older detail data, lightly summarized data, highly summarized data, and metadata. The architecture of a data warehouse includes a backend process (the extraction of data from source systems), the warehouse, and the front-end use (the accessing of data from the warehouse). A data mart is a smaller version of a data warehouse that supports the narrower set of requirements of a single business unit. Data marts should be developed in an integrated manner in order to avoid repeating the "silos of information" problem.An operational data store is a database for transaction processing systems that uses the data warehouse approach to provide clean data. Data warehousing is constantly changing, with the associated opportunities for practice and research, such as the potential for knowledge management using the warehouse.

Automatically generating personalized user interfaces
Data warehouses contain large amounts of information, often collected from a variety of independent sources. Decision-support functions in a warehouse, such as on-line analytical processing (OLAP), involve hundreds of complex aggregate queries over large volumes of data. It is not feasible to compute these queries by scanning the data sets each time. Warehouse applications therefore build a large number of summary tables, or materialized aggregate views, to help them increase the system performance. As changes, most notably new transactional data, are collected at the data sources, all summary tables at the warehouse that depend upon this data need to be updated. Usually, source changes are loaded into the warehouse at regular intervals, usually once a day, in a batch window, and the warehouse is made unavailable for querying while it is updated. Since the number of summary tables that need to be maintained is often large, a critical issue for data warehousing is how to maintain the summary tables efficiently. In this paper we propose a method of maintaining aggregate views (the summary-delta table method), and use it to solve two problems in maintaining summary tables in a warehouse: (1) how to efficiently maintain a summary table while minimizing the batch window needed for maintenance, and (2) how to maintain a large set of summary tables defined over the same base tables. While several papers have addressed the issues relating to choosing and materializing a set of summary tables, this is the first paper to address maintaining summary tables efficiently.

Finger mouse and gesture recognition system as a new human computer interface
Null

HybridTouch: an intuitive manipulation technique for PDAs using their front and rear surfaces
Data warehouse refreshment is often viewed as a problem of maintaining materialized views over operational sources. In this paper, we show that the data warehouse refreshment process is a complex process comprising several tasks, e.g.,monitoring, extracting, transforming, integrating and cleaning operational data, deriving new data, building histories and loading the data warehouse. We propose a novel approach for defining and executing the refreshment process based on specifications stored in an object-oriented metadata repository. Our approach considers the multidimensional character of OLAP data and can be used in conjunction with various operational sources and target data warehouses.

Customization and flexibility in the exmh mail user interface
Data compression is an effective technique to improve the performance of data warehouses. Aggregation and cube are important operations for on-line analytical processing (OLAP). It is a major challenge to develop efficient algorithms for aggregation and cube operations on compressed data warehouses. Many efficient algorithms to compute aggregation and cube for relational OLAP have been developed. Some work has been done on efficiently computing aggregation and cube for multidimensional data warehouses (MDWs) that store datasets in multidimensional arrays rather than in tables. However, to our knowledge, there is few to date in the literature describing aggregation algorithms on compressed data warehouses for multidimensional OLAP.

A Wizard of Oz study for an AR multimodal interface
Null

Whadget: animation using perceptual interaction through personified hand gestures
Three books that set the standard with their groundbreaking methods and techniques for data warehousing. Great value! Save $20, value of the set is $165. This set includes the following books from the Kimball Group: The Data Warehouse Toolkit, Second Edition This authoritative guide presents clear-cut guidelines for designing dimensional models through the use of real-world data warehouse case studies drawn from a variety of business application areas and industries. Ralph Kimball and Margy Ross help you master the full range of powerful techniques for designing dimensional databases that are easy to understand and provide fast query responses. The Data Warehouse Lifecycle Toolkit, Second Edition With this second edition, Ralph Kimball and his Kimball Group colleagues refined the original set of Lifecycle methods and techniques based on their consulting and training experiences. They walk you through detailed steps of designing, developing, and deploying a data warehousing/business intelligence system, from business requirements gathering through delivery, with the steadfast goal of enabling users to make better, more informed business decisions. The Data Warehouse ETL Toolkit Serving as a road map for planning, designing, building, and running the backroom of a data warehouse, this book provides complete coverage of proven, time-saving ETL techniques. You'll discover how a properly designed ETL system extracts data from the source systems, enforces data quality and consistency standards, conforms the data so that separate sources can be used together, and finally delivers the data in a presentation-ready format

Multifunctional cursor for direct manipulation user interfaces
Systems for managing and querying semistructured-data sources often store data in proprietary object repositories or in a tagged-text format. We describe a technique that can use relational database management systems to store and manage semistructured data. Our technique relies on a mapping between the semistructured data model and the relational data model, expressed in a query language called STORED. When a semistructured data instance is given, a STORED mapping can be generated automatically using data-mining techniques. We are interested in applying STORED to XML data, which is an instance of semistructured data. We show how a document-type-descriptor (DTD), when present, can be exploited to further improve performance.


Topic 88
OMR: An Opportunistic Multi-Path Reliable Routing Protocol in Wireless Sensor Networks
With increasing amount of data being stored in XML format, OLAP queries over these data become important. OLAP queries have been well studied in the relational database systems. However, the evaluation of OLAP queries over XML data is not a trivial extension of the relational solutions, especially when a schema is not available. In this paper, we introduce the IX-cube (Iceberg XML cube) over XML data to tackle the problem. We extend OLAP operations to XML data. We also develop efficient approaches to IX-Cube computation and OLAP query evaluation using IX-cubes.

LSMR: A Label Switching Multipath Routing Protocol for Ad Hoc Networks
Data warehouses using a multidimensional view of data have become very popular in both business and science in recent years. Data warehouses for scientific purposes such as medicine and bio-chemistry1 pose several great challenges to existing data warehouse technology. Data warehouses usually use pre-aggregated data to ensure fast query response. However, pre-aggregation cannot be used in practice if the dimension structures or the relationships between facts and dimensions are irregular. A technique for overcoming this limitation and some experimental results are presented. Queries over scientific data warehouses often need to reference data that is external to the data warehouse, e.g., data that is too complex to be handled by current data warehouse technology, data that is ”owned” by other organizations, or data that is updated frequently. This paper presents a federation architecture that allows the integration of multidimensional warehouse data with complex external data.

A label-switching packet forwarding architecture for multi-hop wireless LANs
Many large organizations have developed data warehouses to support decision making. The data in a warehouse are subject oriented, integrated, time variant, and nonvolatile. A data warehouse contains five types of data: current detail data, older detail data, lightly summarized data, highly summarized data, and metadata. The architecture of a data warehouse includes a backend process (the extraction of data from source systems), the warehouse, and the front-end use (the accessing of data from the warehouse). A data mart is a smaller version of a data warehouse that supports the narrower set of requirements of a single business unit. Data marts should be developed in an integrated manner in order to avoid repeating the "silos of information" problem.An operational data store is a database for transaction processing systems that uses the data warehouse approach to provide clean data. Data warehousing is constantly changing, with the associated opportunities for practice and research, such as the potential for knowledge management using the warehouse.

A Transmission Power Control MAC Protocol for Wireless Sensor Networks
Data warehouses contain large amounts of information, often collected from a variety of independent sources. Decision-support functions in a warehouse, such as on-line analytical processing (OLAP), involve hundreds of complex aggregate queries over large volumes of data. It is not feasible to compute these queries by scanning the data sets each time. Warehouse applications therefore build a large number of summary tables, or materialized aggregate views, to help them increase the system performance. As changes, most notably new transactional data, are collected at the data sources, all summary tables at the warehouse that depend upon this data need to be updated. Usually, source changes are loaded into the warehouse at regular intervals, usually once a day, in a batch window, and the warehouse is made unavailable for querying while it is updated. Since the number of summary tables that need to be maintained is often large, a critical issue for data warehousing is how to maintain the summary tables efficiently. In this paper we propose a method of maintaining aggregate views (the summary-delta table method), and use it to solve two problems in maintaining summary tables in a warehouse: (1) how to efficiently maintain a summary table while minimizing the batch window needed for maintenance, and (2) how to maintain a large set of summary tables defined over the same base tables. While several papers have addressed the issues relating to choosing and materializing a set of summary tables, this is the first paper to address maintaining summary tables efficiently.

Source Routing for Overlay Multicast in Wireless Ad hoc and Sensor Networks
Null

A self-healing On-demand Geographic Path Routing Protocol for mobile ad-hoc networks
Data warehouse refreshment is often viewed as a problem of maintaining materialized views over operational sources. In this paper, we show that the data warehouse refreshment process is a complex process comprising several tasks, e.g.,monitoring, extracting, transforming, integrating and cleaning operational data, deriving new data, building histories and loading the data warehouse. We propose a novel approach for defining and executing the refreshment process based on specifications stored in an object-oriented metadata repository. Our approach considers the multidimensional character of OLAP data and can be used in conjunction with various operational sources and target data warehouses.

PMAC: An Adaptive Energy-Efficient MAC Protocol for Wireless Sensor Networks
Data compression is an effective technique to improve the performance of data warehouses. Aggregation and cube are important operations for on-line analytical processing (OLAP). It is a major challenge to develop efficient algorithms for aggregation and cube operations on compressed data warehouses. Many efficient algorithms to compute aggregation and cube for relational OLAP have been developed. Some work has been done on efficiently computing aggregation and cube for multidimensional data warehouses (MDWs) that store datasets in multidimensional arrays rather than in tables. However, to our knowledge, there is few to date in the literature describing aggregation algorithms on compressed data warehouses for multidimensional OLAP.

An average velocity-based routing protocol with low end-to-end delay for wireless sensor networks
Null

Maximization of energy efficiency in wireless ad hoc and sensor networks with SERENA
Three books that set the standard with their groundbreaking methods and techniques for data warehousing. Great value! Save $20, value of the set is $165. This set includes the following books from the Kimball Group: The Data Warehouse Toolkit, Second Edition This authoritative guide presents clear-cut guidelines for designing dimensional models through the use of real-world data warehouse case studies drawn from a variety of business application areas and industries. Ralph Kimball and Margy Ross help you master the full range of powerful techniques for designing dimensional databases that are easy to understand and provide fast query responses. The Data Warehouse Lifecycle Toolkit, Second Edition With this second edition, Ralph Kimball and his Kimball Group colleagues refined the original set of Lifecycle methods and techniques based on their consulting and training experiences. They walk you through detailed steps of designing, developing, and deploying a data warehousing/business intelligence system, from business requirements gathering through delivery, with the steadfast goal of enabling users to make better, more informed business decisions. The Data Warehouse ETL Toolkit Serving as a road map for planning, designing, building, and running the backroom of a data warehouse, this book provides complete coverage of proven, time-saving ETL techniques. You'll discover how a properly designed ETL system extracts data from the source systems, enforces data quality and consistency standards, conforms the data so that separate sources can be used together, and finally delivers the data in a presentation-ready format

An Energy-Aware Routing Scheme with Node Relay Willingness in Wireless Sensor Networks
Systems for managing and querying semistructured-data sources often store data in proprietary object repositories or in a tagged-text format. We describe a technique that can use relational database management systems to store and manage semistructured data. Our technique relies on a mapping between the semistructured data model and the relational data model, expressed in a query language called STORED. When a semistructured data instance is given, a STORED mapping can be generated automatically using data-mining techniques. We are interested in applying STORED to XML data, which is an instance of semistructured data. We show how a document-type-descriptor (DTD), when present, can be exploited to further improve performance.


Topic 91
Automatic reconstruction of 3D human motion pose from uncalibrated monocular video sequences based on markerless human motion tracking
With increasing amount of data being stored in XML format, OLAP queries over these data become important. OLAP queries have been well studied in the relational database systems. However, the evaluation of OLAP queries over XML data is not a trivial extension of the relational solutions, especially when a schema is not available. In this paper, we introduce the IX-cube (Iceberg XML cube) over XML data to tackle the problem. We extend OLAP operations to XML data. We also develop efficient approaches to IX-Cube computation and OLAP query evaluation using IX-cubes.

Catadioptric Stereo Using Planar Mirrors
Data warehouses using a multidimensional view of data have become very popular in both business and science in recent years. Data warehouses for scientific purposes such as medicine and bio-chemistry1 pose several great challenges to existing data warehouse technology. Data warehouses usually use pre-aggregated data to ensure fast query response. However, pre-aggregation cannot be used in practice if the dimension structures or the relationships between facts and dimensions are irregular. A technique for overcoming this limitation and some experimental results are presented. Queries over scientific data warehouses often need to reference data that is external to the data warehouse, e.g., data that is too complex to be handled by current data warehouse technology, data that is ”owned” by other organizations, or data that is updated frequently. This paper presents a federation architecture that allows the integration of multidimensional warehouse data with complex external data.

Space-Time Invariants for Recognizing 3D Motions from Arbitrary Viewpoints under Perspective Projection
Many large organizations have developed data warehouses to support decision making. The data in a warehouse are subject oriented, integrated, time variant, and nonvolatile. A data warehouse contains five types of data: current detail data, older detail data, lightly summarized data, highly summarized data, and metadata. The architecture of a data warehouse includes a backend process (the extraction of data from source systems), the warehouse, and the front-end use (the accessing of data from the warehouse). A data mart is a smaller version of a data warehouse that supports the narrower set of requirements of a single business unit. Data marts should be developed in an integrated manner in order to avoid repeating the "silos of information" problem.An operational data store is a database for transaction processing systems that uses the data warehouse approach to provide clean data. Data warehousing is constantly changing, with the associated opportunities for practice and research, such as the potential for knowledge management using the warehouse.

Self-Calibration of a 1D Projective Camera and Its Application to the Self-Calibration of a 2D Projective Camera
Data warehouses contain large amounts of information, often collected from a variety of independent sources. Decision-support functions in a warehouse, such as on-line analytical processing (OLAP), involve hundreds of complex aggregate queries over large volumes of data. It is not feasible to compute these queries by scanning the data sets each time. Warehouse applications therefore build a large number of summary tables, or materialized aggregate views, to help them increase the system performance. As changes, most notably new transactional data, are collected at the data sources, all summary tables at the warehouse that depend upon this data need to be updated. Usually, source changes are loaded into the warehouse at regular intervals, usually once a day, in a batch window, and the warehouse is made unavailable for querying while it is updated. Since the number of summary tables that need to be maintained is often large, a critical issue for data warehousing is how to maintain the summary tables efficiently. In this paper we propose a method of maintaining aggregate views (the summary-delta table method), and use it to solve two problems in maintaining summary tables in a warehouse: (1) how to efficiently maintain a summary table while minimizing the batch window needed for maintenance, and (2) how to maintain a large set of summary tables defined over the same base tables. While several papers have addressed the issues relating to choosing and materializing a set of summary tables, this is the first paper to address maintaining summary tables efficiently.

3D Reconstruction of Background and Objects Moving on Ground Plane Viewed from a Moving Camera
Null

Omnistereo: Panoramic Stereo Imaging
Data warehouse refreshment is often viewed as a problem of maintaining materialized views over operational sources. In this paper, we show that the data warehouse refreshment process is a complex process comprising several tasks, e.g.,monitoring, extracting, transforming, integrating and cleaning operational data, deriving new data, building histories and loading the data warehouse. We propose a novel approach for defining and executing the refreshment process based on specifications stored in an object-oriented metadata repository. Our approach considers the multidimensional character of OLAP data and can be used in conjunction with various operational sources and target data warehouses.

Two-Way Ambiguity in 2D Projective Reconstruction from Three Uncalibrated 1D Images
Data compression is an effective technique to improve the performance of data warehouses. Aggregation and cube are important operations for on-line analytical processing (OLAP). It is a major challenge to develop efficient algorithms for aggregation and cube operations on compressed data warehouses. Many efficient algorithms to compute aggregation and cube for relational OLAP have been developed. Some work has been done on efficiently computing aggregation and cube for multidimensional data warehouses (MDWs) that store datasets in multidimensional arrays rather than in tables. However, to our knowledge, there is few to date in the literature describing aggregation algorithms on compressed data warehouses for multidimensional OLAP.

Rigid and Articulated Motion Seen with an Uncalibrated Stereo Rig
Null

Complemental Use of Multiple Cameras for Stable Tracking of Multiple Markers
Three books that set the standard with their groundbreaking methods and techniques for data warehousing. Great value! Save $20, value of the set is $165. This set includes the following books from the Kimball Group: The Data Warehouse Toolkit, Second Edition This authoritative guide presents clear-cut guidelines for designing dimensional models through the use of real-world data warehouse case studies drawn from a variety of business application areas and industries. Ralph Kimball and Margy Ross help you master the full range of powerful techniques for designing dimensional databases that are easy to understand and provide fast query responses. The Data Warehouse Lifecycle Toolkit, Second Edition With this second edition, Ralph Kimball and his Kimball Group colleagues refined the original set of Lifecycle methods and techniques based on their consulting and training experiences. They walk you through detailed steps of designing, developing, and deploying a data warehousing/business intelligence system, from business requirements gathering through delivery, with the steadfast goal of enabling users to make better, more informed business decisions. The Data Warehouse ETL Toolkit Serving as a road map for planning, designing, building, and running the backroom of a data warehouse, this book provides complete coverage of proven, time-saving ETL techniques. You'll discover how a properly designed ETL system extracts data from the source systems, enforces data quality and consistency standards, conforms the data so that separate sources can be used together, and finally delivers the data in a presentation-ready format

Multiple Motion Scene Reconstruction with Uncalibrated Cameras
Systems for managing and querying semistructured-data sources often store data in proprietary object repositories or in a tagged-text format. We describe a technique that can use relational database management systems to store and manage semistructured data. Our technique relies on a mapping between the semistructured data model and the relational data model, expressed in a query language called STORED. When a semistructured data instance is given, a STORED mapping can be generated automatically using data-mining techniques. We are interested in applying STORED to XML data, which is an instance of semistructured data. We show how a document-type-descriptor (DTD), when present, can be exploited to further improve performance.

